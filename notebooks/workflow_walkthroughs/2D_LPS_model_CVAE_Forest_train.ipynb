{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Variational Autoencoder (CVAE) is expected to be able to encode various features of complex bunch configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload when refreshing notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# import Python functions \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from Python_Functions.functions import cropProfmonImg, matstruct_to_dict, extractDAQBSAScalars, segment_centroids_and_com, plot2DbunchseparationVsCollimatorAndBLEN, extract_processed_images, apply_centroid_correction,apply_tcav_zeroing_filter, MLP\n",
    "from Python_Functions.cvae import CVAE, vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat \n",
    "import re\n",
    "import os \n",
    "# Assumed: commonIndexFromSteps, extractDAQBSAScalars, and other helper functions are available\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Define the list of (experiment, runname, step_identifier) pairs to load\n",
    "# ----------------------------------------------------------------------\n",
    "run_pairs = [\n",
    "    ('E300', '12431', 1),  # Example pairs, modify this list\n",
    "    ('E300', '12427', 1),\n",
    "    #('E300', '12431', 1),\n",
    "    #('E338', '12710', 1)\n",
    "    # Add more pairs here...\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Initialize lists for concatenation\n",
    "# ----------------------------------------------------------------------\n",
    "all_images = []\n",
    "all_predictors = []\n",
    "all_indices = []\n",
    "\n",
    "print(\"Starting multi-run data loading and concatenation...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Loop through runs, load data, and concatenate\n",
    "# ----------------------------------------------------------------------\n",
    "charge_merged = []\n",
    "for experiment, runname, step_id in run_pairs:\n",
    "    \n",
    "    # --- A. Load Processed LPSImage Data and Good Shots Index ---\n",
    "    pickle_filename = f'../../data/processed/LPSImage_goodshots_{experiment}_{runname}_{step_id}.pkl'\n",
    "\n",
    "    try:\n",
    "        with open(pickle_filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        LPSImage_good = data['LPSImage'] # Filtered LPS images\n",
    "        # This 'goodShots' index is relative to the phase-filtered data (all_idx).\n",
    "        goodShots_scal_common_index = data['scalarCommonIndex'] \n",
    "        \n",
    "        print(f\"Loaded {experiment}_{runname}: LPSImage shape {LPSImage_good.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: Pickle file not found at {pickle_filename}\")\n",
    "        continue\n",
    "    \n",
    "    # --- B. Load and Filter Predictor Data (BSA Scalars) ---\n",
    "    \n",
    "    # 1. Load data_struct\n",
    "    dataloc = f'../../data/raw/{experiment}/{experiment}_{runname}/{experiment}_{runname}.mat'\n",
    "    try:\n",
    "        mat = loadmat(dataloc,struct_as_record=False, squeeze_me=True)\n",
    "        data_struct = mat['data_struct']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: .mat file not found at {dataloc}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Extract full BSA scalars (filtered by step_list if needed)\n",
    "    # Don't filter by common index here, we'll do it with the goodShots scalar common index loaded from the file\n",
    "    bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct, filter_index=False)\n",
    "    bsaScalarData = apply_tcav_zeroing_filter(bsaScalarData, bsaVars)\n",
    "\n",
    "    # 3. \n",
    "\n",
    "    # 5. Filter BSA data using the final index\n",
    "    # goodShots_scal_common_index is 1 based indexing from MATLAB, convert to 0 based\n",
    "    bsaScalarData_filtered = bsaScalarData[:, goodShots_scal_common_index - 1]\n",
    "    \n",
    "    isChargePV = [bool(re.search(r'TORO_LI20_2452_TMIT', pv)) for pv in bsaVars]\n",
    "    if isChargePV:\n",
    "        # Extract charge data\n",
    "        pvidx = [i for i, val in enumerate(isChargePV) if val]\n",
    "        charge = bsaScalarData[pvidx, :][0] * 1.6e-19  # in C \n",
    "        charge_filtered = charge[goodShots_scal_common_index - 1]\n",
    "    # 6. Construct the predictor array\n",
    "    predictor_current = np.vstack(bsaScalarData_filtered).T\n",
    "    \n",
    "    # C. Append to master lists\n",
    "    all_images.append(LPSImage_good)\n",
    "    all_predictors.append(predictor_current)\n",
    "    charge_merged.append(charge_filtered)\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Concatenate and finalize arrays\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Set image half dimensions (should match preprocessing)\n",
    "yrange = 100\n",
    "xrange = 100\n",
    "\n",
    "# Combine all data arrays from the runs\n",
    "images_tmp = np.concatenate(all_images, axis=0)\n",
    "images_tmp = images_tmp.reshape(images_tmp.shape[0], 2*yrange, 2*xrange)\n",
    "predictor_tmp = np.concatenate(all_predictors, axis=0)\n",
    "charge = np.concatenate(charge_merged, axis=0)\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Concatenated Data Shapes ---\")\n",
    "print(f\"Total LPS Images (images): {images_tmp.shape}\")\n",
    "print(f\"Total Predictors (predictor): {predictor_tmp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase selection, flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampl_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_A' in var)\n",
    "xtcavAmpl = predictor_tmp[:, ampl_idx]\n",
    "\n",
    "phase_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_P' in var)\n",
    "xtcavPhase = predictor_tmp[:, phase_idx]\n",
    "xtcavOffShots = xtcavAmpl<0.1\n",
    "xtcavPhase[xtcavOffShots] = 0 #Set this for ease of plotting\n",
    "# \n",
    "near_minus_90_idx = np.where((xtcavPhase >= -90.55) & (xtcavPhase <= -89.55))[0]\n",
    "near_plus_90_idx = np.where((xtcavPhase >= 89.55) & (xtcavPhase <= 90.55))[0]\n",
    "lps_idx = near_minus_90_idx.tolist() + near_plus_90_idx.tolist()\n",
    "# Flip image horizontally for -90 deg phase\n",
    "images_flipped = images_tmp.copy()\n",
    "images_flipped[near_minus_90_idx, :, :] = np.flip(images_tmp[near_minus_90_idx, :, :], axis=2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(xtcavAmpl, label='Amplitude', color='b')\n",
    "ax1.set_ylabel('XTCAV Ampl [MV]', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xtcavPhase, label='Phase', color='r')\n",
    "ax2.set_ylabel('XTCAV Phase [deg]', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.title('XTCAV Amplitude and Phase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude BSA Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Python_Functions.functions import exclude_bsa_vars\n",
    "excluded_var_idx = exclude_bsa_vars(bsaVars)\n",
    "predictor_tmp_cleaned = np.delete(predictor_tmp, excluded_var_idx, axis=1)[lps_idx, :]\n",
    "print(f\"Predictor shape after excluding variables: {predictor_tmp_cleaned.shape}\")\n",
    "\n",
    "LPSimg = images_flipped[lps_idx]\n",
    "charge_filtered = charge[lps_idx]\n",
    "print(f\"LPS Image shape after filtering: {LPSimg.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XTCAV calibration\n",
    "krf = 239.26\n",
    "cal = 1167 # um/deg  http://physics-elog.slac.stanford.edu/facetelog/show.jsp?dir=/2025/11/13.03&pos=2025-$\n",
    "streakFromGUI = cal*krf*180/np.pi*1e-6#um/um\n",
    "xtcalibrationfactor = data_struct.metadata.DTOTR2.RESOLUTION*1e-6/streakFromGUI/3e8\n",
    "# Flags\n",
    "# If enabled, GMM fit is weighted to predict better current profile rather than overall image fit\n",
    "do_current_profile = True\n",
    "\n",
    "# Number of components for the CVAE.\n",
    "# Too low, the model will not capture the complexity of the data.\n",
    "# Too high, the prediction task becomes too difficult.\n",
    "NCOMP = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_struct.scalars.nonBSA_List_S20Magnets.LI20_XCOR_3086_BACT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 2*xrange\n",
    "INPUT_CHANNELS = 1 # Assuming LPSimg is grayscale/single-channel data\n",
    "# 1. Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# 2. Initialize Model and Optimizer\n",
    "model_cvae = CVAE(latent_dim=NCOMP).to(device)\n",
    "optimizer = torch.optim.Adam(model_cvae.parameters(), lr=1e-3)\n",
    "\n",
    "# LPSimg is a numpy array of shape [n_samples, 200, 200].\n",
    "BATCH_SIZE = 16\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Convert numpy data to PyTorch Tensor, add the Channel dimension (C=1)\n",
    "LPSimg_tensor = torch.from_numpy(LPSimg).unsqueeze(1).float()\n",
    "LPSimg_tensor /= LPSimg_tensor.max()\n",
    "# Create a simple DataLoader for the data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(LPSimg_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. Training Loop\n",
    "N_EPOCHS = 7\n",
    "print(\"\\nStarting training loop ({} epochs)...\".format(N_EPOCHS))\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstruction, mu, logvar = model_cvae(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = vae_loss(reconstruction, data, mu, logvar)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{N_EPOCHS}, Average VAE Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "\"\"\"\n",
    "Creates an interactive plot with a slider to compare original and \n",
    "reconstructed images from the VAE.\n",
    "\"\"\"\n",
    "model_cvae.eval()\n",
    "\n",
    "def plot_cvae_image_pred(idx):\n",
    "    # Set up the figure and two subplots for the images\n",
    "    fig, (ax_orig, ax_recon) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "\n",
    "    initial_data = LPSimg[idx] / LPSimg[idx].max()\n",
    "    input_data = torch.from_numpy(initial_data).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstruction, _, _ = model_cvae(input_data)\n",
    "\n",
    "    # Remove channel dimension and convert to numpy for plotting\n",
    "    img_orig = initial_data\n",
    "    img_recon = reconstruction.squeeze().cpu().numpy()\n",
    "\n",
    "    # Determine common vmin/vmax for consistent color mapping across both images\n",
    "    v_min = min(img_orig.min(), img_recon.min())\n",
    "    v_max = max(img_orig.max(), img_recon.max())\n",
    "\n",
    "    # Display original image\n",
    "    im1 = ax_orig.imshow(img_orig, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "    ax_orig.set_title(f'Original Image (Index {idx})')\n",
    "    plt.colorbar(im1, ax=ax_orig)\n",
    "\n",
    "    # Display reconstructed image\n",
    "    im2 = ax_recon.imshow(img_recon, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "    ax_recon.set_title(f'Reconstructed Image (Index {idx})')\n",
    "    plt.colorbar(im2, ax=ax_recon)\n",
    "# Create slider\n",
    "interact(plot_cvae_image_pred, idx=IntSlider(min=0, max=LPSimg.shape[0]-1, step=1, value=0, layout=Layout(width='80%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, Layout\n",
    "idx = 120\n",
    "def plot_cvae_image_pred(**slider_values):\n",
    "    # Create latent vector from slider values\n",
    "    mu = np.array([slider_values[f\"Z_{i+1}\"] for i in range(NCOMP)])\n",
    "    mu_tensor = torch.from_numpy(mu).unsqueeze(0).float().to(device)\n",
    "    # Set up the figure and two subplots for the images\n",
    "    fig, (ax_orig, ax_recon, ax_modified_recon) = plt.subplots(1, 3, figsize=(14, 5))\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "\n",
    "    initial_data = LPSimg[idx] / LPSimg[idx].max()\n",
    "    input_data = torch.from_numpy(initial_data).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstruction, _, _ = model_cvae(input_data)\n",
    "\n",
    "    # Remove channel dimension and convert to numpy for plotting\n",
    "    img_orig = initial_data\n",
    "    img_recon = reconstruction.squeeze().cpu().numpy()\n",
    "\n",
    "    # Determine common vmin/vmax for consistent color mapping across both images\n",
    "    v_min = min(img_orig.min(), img_recon.min())\n",
    "    v_max = max(img_orig.max(), img_recon.max())\n",
    "\n",
    "    # Display original image\n",
    "    im1 = ax_orig.imshow(img_orig, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "    ax_orig.set_title(f'Original Image (Index {idx})')\n",
    "    plt.colorbar(im1, ax=ax_orig)\n",
    "\n",
    "    # Display reconstructed image\n",
    "    im2 = ax_recon.imshow(img_recon, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "    ax_recon.set_title(f'Reconstructed Image (Index {idx})')\n",
    "\n",
    "    plt.colorbar(im2, ax=ax_recon)\n",
    "\n",
    "    # Display modified reconstructed image\n",
    "    modified_reconstruction = model_cvae.decode_latent_mu(mu_tensor)\n",
    "    img_modified_recon = modified_reconstruction.squeeze().detach().numpy()\n",
    "    im3 = ax_modified_recon.imshow(img_modified_recon, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "    ax_modified_recon.set_title(f'Modified Reconstruction (Index {idx})')\n",
    "\n",
    "    plt.colorbar(im3, ax=ax_modified_recon)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "\n",
    "N_SAMPLES = LPSimg.shape[0]\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "initial_data = LPSimg[idx] / LPSimg[idx].max()\n",
    "input_data = torch.from_numpy(initial_data).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "initial_value = model_cvae.generate_latent_mu(input_data).detach().numpy()[0]\n",
    "for i in range(NCOMP):\n",
    "    pv_name = f\"Z_{i+1}\"\n",
    "    pv_index = i\n",
    "    min_val, max_val = -5.0, 5.0\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value[i],\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_cvae_image_pred, **widget_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all LPS images to latent z parameters.\n",
    "latent_z_array = np.zeros((LPSimg.shape[0], NCOMP )) \n",
    "for i in range(LPSimg.shape[0]):\n",
    "    mu_tensor = model_cvae.generate_latent_mu(torch.from_numpy(LPSimg[i]/LPSimg[i].max()).unsqueeze(0).unsqueeze(0).float().to(device))\n",
    "    latent_z_array[i] = mu_tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows are valid in this mock example\n",
    "valid_rows = [True for i in range(latent_z_array.shape[0])]\n",
    "predictor_filtered = predictor_tmp_cleaned[valid_rows]\n",
    "biGaussian_params_array_filtered = latent_z_array[valid_rows]\n",
    "print(f\"After removing invalid rows, dataset shape: Predictors {predictor_filtered.shape}, Bi-Gaussian Params {biGaussian_params_array_filtered.shape}\")\n",
    "\n",
    "# --- Original scaling and splitting logic follows ---\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "iz_scaler = MinMaxScaler()\n",
    "x_scaled = x_scaler.fit_transform(predictor_filtered)\n",
    "Iz_scaled = iz_scaler.fit_transform(biGaussian_params_array_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# 80/20 train-test split\n",
    "x_train_full, x_test_scaled, Iz_train_full, Iz_test_scaled, ntrain, ntest = train_test_split(\n",
    "    x_scaled, Iz_scaled, np.arange(Iz_scaled.shape[0]), test_size=0.2, random_state = 42)\n",
    "\n",
    "# 20% validation split \n",
    "x_train_scaled, x_validation, Iz_train_scaled, y_validation = train_test_split(\n",
    "    x_train_full, Iz_train_full, test_size=0.2, random_state = 42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_validation = torch.tensor(x_validation, dtype=torch.float32)\n",
    "X_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Iz_train_scaled, dtype=torch.float32)\n",
    "y_validation = torch.tensor(y_validation, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Iz_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(X_train, Y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "\n",
    "# --- 2. Model and Hyperparameter Setup ---\n",
    "# The Random Forest is initialized with its structure (number of trees, depth, etc.)\n",
    "# This replaces the PyTorch MLP class definition.\n",
    "\n",
    "print(\"\\n--- Initializing Model ---\")\n",
    "# Equivalent to: model = MLP(X_train.shape[1], Y_train.shape[1])\n",
    "# n_estimators is equivalent to the overall model complexity/capacity\n",
    "# max_depth controls the depth, similar to the number of layers/nodes.\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=500,        # Number of trees (like epochs/steps, higher = more complex)\n",
    "    max_depth=15,            # Max depth of trees (limits complexity)\n",
    "    min_samples_leaf=5,      # Regularization/pruning parameter\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # Use all available cores\n",
    ")\n",
    "\n",
    "# Note: In Random Forest, there is no separate 'optimizer' or 'learning rate'\n",
    "# as the training is done via deterministic tree growing (not gradient descent).\n",
    "\n",
    "\n",
    "# --- 4. Training and Evaluation (Equivalent to the Training Loop) ---\n",
    "# Random Forest is trained in a single 'fit' call, not in epochs.\n",
    "# We mimic the training block structure and calculate loss/metrics.\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"\\n--- Starting Model Fitting (One Shot) ---\")\n",
    "\n",
    "# Fit the model (This is the entire 'training loop' for RF)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "# 1. Training Set Evaluation\n",
    "Y_train_pred = model.predict(X_train)\n",
    "train_mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "\n",
    "# 2. Validation Set Evaluation\n",
    "Y_val_pred = model.predict(x_validation)\n",
    "val_mse = mean_squared_error(y_validation, Y_val_pred)\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Results ---\")\n",
    "print(f\"Total Fitting Time: {t1 - t0:.2f} seconds\")\n",
    "print(f\"Final Train MSE: {train_mse:.6f}\")\n",
    "print(f\"Final Validation MSE: {val_mse:.6f}\")\n",
    "\n",
    "# To see the importance of the input features:\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "for i, importance in enumerate(model.feature_importances_):\n",
    "    print(f\"Feature {i} importance: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Evaluate model\n",
    "pred_train_scaled = model.predict(X_train)\n",
    "pred_test_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "pred_train_full = iz_scaler.inverse_transform(pred_train_scaled)\n",
    "pred_test_full = iz_scaler.inverse_transform(pred_test_scaled)\n",
    "Iz_train_true = iz_scaler.inverse_transform(Iz_train_scaled)\n",
    "Iz_test_true = iz_scaler.inverse_transform(Iz_test_scaled)\n",
    "elapsed = time.time() - t0\n",
    "print(\"Elapsed time [mins] = {:.1f} \".format(elapsed/60))\n",
    "\n",
    "# Compute R² score\n",
    "def r2_score(true, pred):\n",
    "    RSS = np.sum((true - pred)**2)\n",
    "    TSS = np.sum((true - np.mean(true))**2)\n",
    "    return 1 - RSS / TSS if TSS != 0 else s0\n",
    "\n",
    "# Compute R² on scaled data, instead of the actual bi-Gaussian parameters, to avoid distortion from different scales\n",
    "print(\"Train R²: {:.2f} %\".format(r2_score(Iz_train_scaled.ravel(), pred_train_scaled.ravel()) * 100))\n",
    "print(\"Test R²: {:.2f} %\".format(r2_score(Iz_test_scaled.ravel(), pred_test_scaled.ravel()) * 100))\n",
    "\n",
    "# Plot histogram of R² values for each test sample\n",
    "r2_values = [r2_score(Iz_test_scaled.T.reshape(NCOMP,Iz_test_scaled.shape[0])[:,i], pred_test_scaled.T.reshape(NCOMP,Iz_test_scaled.shape[0])[:,i]) for i in range(Iz_test_scaled.shape[0])]\n",
    "# Throw away values outside 0 to 1, and count the number of throws\n",
    "r2_values_new = [r2 for r2 in r2_values if 0 <= r2 <= 1]\n",
    "num_throws = len(r2_values) - len(r2_values_new)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(r2_values_new, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of R² Values for Test Samples')\n",
    "plt.xlabel('R² Value')\n",
    "plt.ylabel(f'Plotted Samples: {len(r2_values) - num_throws} / Total Samples: {len(r2_values)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tmp = 34\n",
    "print(pred_test_scaled[idx_tmp])\n",
    "print(pred_test_full[idx_tmp])\n",
    "print(Iz_train_scaled[idx_tmp])\n",
    "print(Iz_train_true[idx_tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot true vs prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred(idx):\n",
    "    fig, (ax0, ax1, ax2, cx1) = plt.subplots(1,4,figsize=(10, 3), gridspec_kw={'width_ratios': [1, 1, 1, 0.02]})\n",
    "\n",
    "    raw_im = np.flip(LPSimg[valid_rows][ntest[idx]], axis=0)\n",
    "    im0 = ax0.imshow(raw_im, cmap = \"jet\",aspect='auto', vmin = 0, vmax = 400)\n",
    "    ax0.set(ylabel=\"y [pix]\")\n",
    "    ax0.set(xlabel = \"Time [fs]\")\n",
    "    ax0.set(title = f\"Raw Image (Shot Number: {ntest[idx]})\")\n",
    "    ax0.set(xlim = (0,2*xrange))\n",
    "    ax0.set(ylim= (0,2*yrange))\n",
    "\n",
    "\n",
    "    true_params = Iz_test_true.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    true_im = model_cvae.decode_latent_mu(torch.tensor(true_params).unsqueeze(0).float().to(device)).squeeze().cpu().detach().numpy()\n",
    "    true_im = np.flip(true_im, axis=0)\n",
    "    im1 = ax1.imshow(true_im, cmap = \"jet\",aspect='auto', vmin = 0, vmax = 1)\n",
    "\n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = f\"CVAE(Shot Number: {ntest[idx]})\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "    # print(\"True Gaussian Parameters:\")\n",
    "    # true_ug = unflatten_biGaussian_params(true_params)\n",
    "    # print(\"Means (Mu):\", true_ug['mu'].numpy())\n",
    "    # print(\"Covariances (Sigma):\", true_ug['Sigma'].numpy())\n",
    "    # print(\"Weights (Pi):\", true_ug['pi'].numpy())\n",
    "\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params).unsqueeze(0).float().to(device)).squeeze().cpu().detach().numpy()\n",
    "    pred_im = np.flip(pred_im, axis=0)\n",
    "    im2 = ax2.imshow(pred_im, cmap = \"jet\",aspect='auto',vmin = 0, vmax = 1)\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction of CVAE\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(true_params, pred_params)\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='80%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred_current(idx):\n",
    "    FACTOR = 1e15\n",
    "    fig, (ax0, ax1) = plt.subplots(2,1,figsize=(10, 6))\n",
    "\n",
    "    x_grid = np.linspace(-xrange*xtcalibrationfactor*FACTOR, xrange*xtcalibrationfactor*FACTOR, xrange * 2)\n",
    "    raw_im = np.flip(LPSimg[valid_rows][ntest[idx]], axis=0)\n",
    "    raw_im_proj = np.sum(raw_im, axis = 0)\n",
    "    raw_im_proj = raw_im_proj / np.sum(raw_im_proj)\n",
    "    # Measure the peak for translation\n",
    "    peak_index = np.argmax(raw_im_proj)\n",
    "    if isChargePV:\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        raw_im_proj = raw_im_proj * charge_value * FACTOR\n",
    "    ax0.plot(x_grid - (peak_index-xrange) * xtcalibrationfactor * FACTOR, raw_im_proj, label = \"raw\")\n",
    "    ax0.set(ylabel=\"Current [A]\")\n",
    "    ax0.set(xlabel = \"Time [fs]\")\n",
    "    ax0.set(title = f\"Raw Image (Shot Number: {ntest[idx]}) (Total Charge: {charge_value*1e12:.2f} pC)\" if isChargePV else f\"Raw Image (Shot Number: {ntest[idx]})\")\n",
    "\n",
    "    true_params = Iz_test_true.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    \n",
    "    if isChargePV:\n",
    "        # Normalize true image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "    true_im = model_cvae.decode_latent_mu(torch.from_numpy(true_params).float().to(device)).detach().numpy()[0][0]\n",
    "    true_im_proj = np.sum(true_im, axis = 0)  * FACTOR * charge_value / np.sum(true_im)\n",
    "    true_peak_index = np.argmax(true_im_proj)\n",
    "    ax0.plot(x_grid-(true_peak_index-xrange) * xtcalibrationfactor * FACTOR, true_im_proj, label = \"fit\")\n",
    "\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    \n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "    pred_im = model_cvae.decode_latent_mu(torch.from_numpy(pred_params).float().to(device)).detach().numpy()[0][0]\n",
    "         \n",
    "    pred_im_proj = np.sum(pred_im, axis = 0) * FACTOR * charge_value / np.sum(pred_im)\n",
    "    pred_peak_index = np.argmax(pred_im_proj)\n",
    "    ax0.plot(x_grid-(pred_peak_index-xrange) * xtcalibrationfactor * FACTOR, pred_im_proj, label = \"prediction\")\n",
    "    # Fix x-axis limits\n",
    "    ax0.set(xlim = (-xrange * xtcalibrationfactor * FACTOR, xrange * xtcalibrationfactor * FACTOR))\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1.imshow(np.flip(raw_im, axis=0), cmap = \"jet\",aspect='auto', extent = (-xrange * xtcalibrationfactor * FACTOR, xrange * xtcalibrationfactor * FACTOR, 0, 200))\n",
    "\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(true_params, pred_params)\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "    # Display image_to_bigaussian_params debug info\n",
    "    #biGaussianTest = image_to_bigaussian_params(images[valid_rows][ntest[idx]].reshape((2*yrange, 2*xrange)), do_current_profile, debug=True)\n",
    "    #print(\"Bi-Gaussian Parameters for Test Image:\", biGaussianTest)\n",
    "    \n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred_current, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='80%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of PV Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bsaScalarData TCAV_LI20_2400_P and TCAV_LI20_2400_A, most important predictors.\n",
    "bsaVarNames = bsaVars\n",
    "var1_name = 'TCAV_LI20_2400_P'\n",
    "var2_name = 'TCAV_LI20_2400_A'\n",
    "var1_idx = bsaVarNames.index(var1_name)\n",
    "var2_idx = bsaVarNames.index(var2_name)\n",
    "print(f\"Plotting BSA Scalars: {var1_name} (index {var1_idx}) and {var2_name} (index {var2_idx})\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(predictor_tmp[:, var1_idx], predictor_tmp[:, var2_idx], alpha=0.5)\n",
    "plt.xlabel(var1_name)\n",
    "plt.ylabel(var2_name)\n",
    "plt.title('BSA Scalar Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming bsaScalarData has shape (N_variables, N_samples) from your function\n",
    "# Transpose the data so features are columns and samples are rows for scikit-learn PCA\n",
    "X = predictor_tmp_cleaned\n",
    "print(X.shape)\n",
    "bsaVarNames_cleaned = [var for i, var in enumerate(bsaVars) if i not in excluded_var_idx]\n",
    "# Apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Determine the maximum number of components to test\n",
    "# X.shape is (N_samples, N_variables). max_components = min(N_samples, N_variables) - 1 for a stable PCA\n",
    "max_components = min(X.shape) // 5 \n",
    "\n",
    "# Lists to store results\n",
    "n_components_list = []\n",
    "reconstruction_losses = []\n",
    "\n",
    "# Loop through possible number of components\n",
    "for k in range(1, max_components + 1):\n",
    "    # 1. Initialize and fit PCA\n",
    "    pca_study = PCA(n_components=k)\n",
    "    pca_study.fit(X)\n",
    "    \n",
    "    # 2. Transform and Inverse Transform (Reconstruct)\n",
    "    X_reduced = pca_study.transform(X)\n",
    "    X_reconstructed = pca_study.inverse_transform(X_reduced)\n",
    "    \n",
    "    # 3. Calculate Reconstruction Loss (Mean Squared Error)\n",
    "    loss = mean_squared_error(X, X_reconstructed)\n",
    "    \n",
    "    # Store results\n",
    "    n_components_list.append(k)\n",
    "    reconstruction_losses.append(loss)\n",
    "    \n",
    "# 4. Plot the Results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(n_components_list, reconstruction_losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Reconstruction Loss (Mean Squared Error)')\n",
    "# Log scale in y-axis for better visualization\n",
    "plt.yscale('log')\n",
    "plt.title('PCA of BSA Scalars: Reconstruction Loss vs. Number of Components')\n",
    "plt.grid(True)\n",
    "# 5. Identify the \"Elbow\" point visually after plotting plt.show()\n",
    "plt.show() \n",
    "\n",
    "# After plotting, the optimal number of components is the 'elbow' point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_components = 11\n",
    "\n",
    "pca_comp_study = PCA(n_components=num_components)\n",
    "X_pca = pca_comp_study.fit_transform(X)\n",
    "# 1. Get explained variance and component loadings\n",
    "\n",
    "variance_ratios = pca_comp_study.explained_variance_ratio_\n",
    "loadings = pca_comp_study.components_\n",
    "\n",
    "# 2. Create the DataFrame for component composition\n",
    "# Each row in 'loadings' is a principal component (PC)\n",
    "# Each column corresponds to a feature (PV)\n",
    "df_loadings = pd.DataFrame(loadings, columns=bsaVarNames_cleaned)\n",
    "\n",
    "# 3. Add Component labels and Significance\n",
    "component_labels = [f'PC {i+1}' for i in range(num_components)]\n",
    "df_loadings.insert(0, 'Component', component_labels)\n",
    "df_loadings.insert(1, 'Significance (Explained Variance Ratio)', variance_ratios)\n",
    "\n",
    "# 4. Format the output\n",
    "# The components are already ordered by significance (PC 1 is most significant)\n",
    "# Format the significance column as a percentage for clarity\n",
    "df_loadings['Significance (Explained Variance Ratio)'] = \\\n",
    "    df_loadings['Significance (Explained Variance Ratio)'].map(lambda x: f'{x:.4f} ({x*100:.2f}%)')\n",
    "\n",
    "# Format the loadings to a fixed number of decimal places\n",
    "for col in bsaVarNames_cleaned:\n",
    "    df_loadings[col] = df_loadings[col].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "composition_table = df_loadings.to_markdown(index=False)\n",
    "print(\"Composition of Principal Components:\")\n",
    "print(composition_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract and Square the Loadings (Loadings matrix is: Components x Variables)\n",
    "# loadings = pca_comp_study.components_ (from your provided code block)\n",
    "squared_loadings = loadings**2\n",
    "\n",
    "# 2. Sum Across Components (Sum columns to get total significance per variable)\n",
    "# The result is an array where each element is the total squared loading for a variable\n",
    "total_squared_loadings = np.sum(squared_loadings, axis=0)\n",
    "print(total_squared_loadings.shape)\n",
    "# 3. Create a DataFrame for sorting\n",
    "# bsaVars is the list of variable names (features)\n",
    "df_var_significance = pd.DataFrame({\n",
    "    'BSA Variable (PV)': bsaVarNames_cleaned,\n",
    "    'Total Squared Loading (Significance)': total_squared_loadings\n",
    "})\n",
    "\n",
    "# 4. Sort in descending order\n",
    "df_var_significance = df_var_significance.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 5. Specify the number of top variables to display\n",
    "N_top = 20  # Example: display the top few most significant variables\n",
    "\n",
    "# 6. Format and display the table\n",
    "df_top_vars = df_var_significance.head(N_top)\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "print(f\"Top {N_top} BSA Variables Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_var_significance[\n",
    "    ~df_var_significance['BSA Variable (PV)'].str.contains('BPM', case=False, na=False)\n",
    "]\n",
    "\n",
    "# 5. Sort the filtered variables in descending order of significance\n",
    "df_filtered = df_filtered.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 6. Select Top N and format\n",
    "N_top = 20 # Display the top few most significant non-BPM variables\n",
    "df_top_vars = df_filtered.head(N_top)\n",
    "\n",
    "# Format the significance column\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "\n",
    "print(f\"Top {N_top} BSA Variables (EXCLUDING BPMS) Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Layout\n",
    "import torch\n",
    "\n",
    "index_to_examine = 1  # Change this index to examine different test samples\n",
    "\n",
    "# Variables to be controlled (Non-BPM variables from previous ranking logic)\n",
    "CONTROL_VARS = [\n",
    "    'BLEN_LI11_359_BRAW', \n",
    "    'LASR_LT10_930_PWR', 'PMTR_HT10_950_PWR', \n",
    "    'KLYS_LI10_41_FB_FAST_PACT', 'KLYS_LI10_41_FB_FAST_AACT'\n",
    "]\n",
    "# ==============================================================================\n",
    "# --- DYNAMIC PREDICTION FUNCTION ---\n",
    "# ==============================================================================\n",
    "original_predictor_reconstructed = x_scaler.inverse_transform(X_test.numpy())\n",
    "def plot_dynamic_pred(**slider_values):\n",
    "    idx = index_to_examine\n",
    "    fig, (ax1, ax2,ax3, cx1) = plt.subplots(1,4,figsize=(10, 3), gridspec_kw={'width_ratios': [1, 1,1, 0.02]})\n",
    "    \n",
    "    true_params = Iz_test_true.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    true_im = model_cvae.decode_latent_mu(true_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        pred_im_sum = np.sum(true_im)\n",
    "        true_im = true_im / pred_im_sum * charge_value\n",
    "         \n",
    "    im1 = ax1.imshow(true_im, cmap = \"jet\",aspect='auto')\n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = f\"True(Shot Number: {ntest[idx]})\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(pred_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        pred_im_sum = np.sum(pred_im)\n",
    "        pred_im = pred_im / pred_im_sum * charge_value\n",
    "    im2 = ax2.imshow(pred_im, cmap = \"jet\",aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "\n",
    "    # Modify the predictor for the given index based on slider values\n",
    "    modified_predictor = original_predictor_reconstructed[idx].copy()\n",
    "    for var_name, slider in slider_values.items():\n",
    "        pv_index = bsaVarNames_cleaned.index(var_name)\n",
    "        modified_predictor[pv_index] = slider\n",
    "\n",
    "    # Scale back the modified predictor\n",
    "    modified_predictor_scaled = x_scaler.transform(modified_predictor.reshape(1, -1))\n",
    "\n",
    "    # Make prediction with modified predictor\n",
    "    modified_predictor_tensor = torch.tensor(modified_predictor_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        modified_pred_scaled = model(modified_predictor_tensor).numpy()\n",
    "    modified_pred_full = iz_scaler.inverse_transform(modified_pred_scaled)\n",
    "    modified_pred_params = modified_pred_full[0]\n",
    "    print(modified_pred_params)\n",
    "    modified_pred_im = model_cvae.decode_latent_mu(modified_pred_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        modified_pred_im_sum = np.sum(modified_pred_im)\n",
    "        modified_pred_im = modified_pred_im / modified_pred_im_sum * charge_value\n",
    "    im3 = ax3.imshow(modified_pred_im, cmap = \"jet\",aspect='auto')\n",
    "    ax3.set(xlabel = \"Time [fs]\")\n",
    "    ax3.set(ylabel = \"y [pix]\")\n",
    "    ax3.set(title = \"Modified Prediction\")\n",
    "    ax3.set(xlim = (0,2*xrange))\n",
    "    ax3.set(ylim= (0,2*yrange))\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(true_params, pred_params)\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "raw_min_max = {}\n",
    "for i, pv_name in enumerate(CONTROL_VARS):\n",
    "    pv_index = bsaVarNames_cleaned.index(pv_name)\n",
    "    raw_min_max[pv_name] = (predictor_tmp[:, pv_index].min(), predictor_tmp[:, pv_index].max())\n",
    "\n",
    "N_SAMPLES = Iz_test_true.shape[0]\n",
    "\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "for i, pv_name in enumerate(CONTROL_VARS):\n",
    "    min_val, max_val = raw_min_max[pv_name]\n",
    "    pv_index = bsaVarNames_cleaned.index(pv_name)\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    initial_value = original_predictor_reconstructed[index_to_examine, pv_index] # Use the value for the initial index (idx=0)\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value,\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_dynamic_pred, **widget_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_to_examine = 80  # Change this index to examine different test samples\n",
    "\n",
    "# Variables to be controlled (Non-BPM variables from previous ranking logic)\n",
    "# ==============================================================================\n",
    "# --- DYNAMIC PREDICTION FUNCTION ---\n",
    "# ==============================================================================\n",
    "original_predictor_reconstructed = x_scaler.inverse_transform(X_test.numpy())\n",
    "def plot_dynamic_pred(**slider_values):\n",
    "    idx = index_to_examine\n",
    "    fig, (ax1, ax2,ax3, cx1) = plt.subplots(1,4,figsize=(12, 5), gridspec_kw={'width_ratios': [1, 1,1, 0.02]})\n",
    "    \n",
    "    true_params = Iz_test_true.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    true_im = model_cvae.decode_latent_mu(true_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        pred_im_sum = np.sum(true_im)\n",
    "        true_im = true_im / pred_im_sum * charge_value\n",
    "         \n",
    "    im1 = ax1.imshow(true_im, cmap = \"jet\",aspect='auto')\n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = f\"True(Shot Number: {ntest[idx]})\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,Iz_test_true.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(pred_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        pred_im_sum = np.sum(pred_im)\n",
    "        pred_im = pred_im / pred_im_sum * charge_value\n",
    "    im2 = ax2.imshow(pred_im, cmap = \"jet\",aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "\n",
    "    # Modify the predictor for the given index based on slider values\n",
    "    pca_components = []\n",
    "    for var_name, slider in slider_values.items():\n",
    "        # Reconstruct the original predictor values from PCA components\n",
    "        pca_components.append(slider)\n",
    "    pca_components = np.array(pca_components).reshape(1, -1)\n",
    "    modified_predictor = pca_comp_study.inverse_transform(pca_components).flatten()\n",
    "\n",
    "\n",
    "    # Scale back the modified predictor\n",
    "    modified_predictor_scaled = modified_predictor.reshape(1, -1)\n",
    "\n",
    "    # Make prediction with modified predictor\n",
    "    modified_predictor_tensor = torch.tensor(modified_predictor_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        modified_pred_scaled = model(modified_predictor_tensor).numpy()\n",
    "    modified_pred_full = iz_scaler.inverse_transform(modified_pred_scaled)\n",
    "    modified_pred_params = modified_pred_full[0]\n",
    "\n",
    "    # Print the modified parameters minus the original predicted parameters for comparison\n",
    "    print(\"Difference in Parameters from Original Prediction:\")\n",
    "    for i, name in enumerate([\"pi_1\", \"pi_2\", \"mu_1_x\", \"mu_1_y\", \"sigma_1_xx\", \"sigma_1_xy\", \"sigma_1_yy\", \"sigma_2_xx\", \"sigma_2_xy\", \"sigma_2_yy\"]):\n",
    "        diff = modified_pred_params[i] - pred_params[i]\n",
    "        print(f\"{name}: {diff:.4f}\")\n",
    "\n",
    "    modified_pred_im = model_cvae.decode_latent_mu(modified_pred_params)\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[valid_rows][ntest[idx]]\n",
    "        modified_pred_im_sum = np.sum(modified_pred_im)\n",
    "        modified_pred_im = modified_pred_im / modified_pred_im_sum * charge_value\n",
    "    im3 = ax3.imshow(modified_pred_im, cmap = \"jet\",aspect='auto')\n",
    "    ax3.set(xlabel = \"Time [fs]\")\n",
    "    ax3.set(ylabel = \"y [pix]\")\n",
    "    ax3.set(title = \"Modified Prediction\")\n",
    "    ax3.set(xlim = (0,2*xrange))\n",
    "    ax3.set(ylim= (0,2*yrange))\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(true_params, pred_params)\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "\n",
    "N_SAMPLES = Iz_test_true.shape[0]\n",
    "pca_vals = pca_comp_study.transform(X_test.numpy())\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "for i in range(pca_comp_study.components_.shape[0]):\n",
    "    pv_name = f\"PC{i+1}\"\n",
    "    pv_index = i\n",
    "    min_val, max_val = pca_vals[:, pv_index].min(), pca_vals[:, pv_index].max()\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    initial_value = pca_vals[index_to_examine, pv_index] # Use the value for the initial index (idx=0)\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value,\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_dynamic_pred, **widget_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ImportError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scalers\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "time_stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "joblib_file = '../../model/LPS/MLP_LPS_CVAE_Forest_'+experiment+'_'+runname+'_'+time_stamp+'.pkl'  \n",
    "joblib.dump(model, joblib_file)\n",
    "joblib_file_2 = '../../model/LPS/MLP_LPS_CVAE_'+experiment+'_'+runname+'_'+time_stamp+'.pkl' \n",
    "joblib.dump(model_cvae, joblib_file_2) \n",
    "iz_scaler_file = '../../model/LPS/' + experiment + '_' + runname + '_iz_scaler_CVAE_'+time_stamp+'.pkl'\n",
    "pickle.dump(iz_scaler, open(iz_scaler_file,\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtcav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
