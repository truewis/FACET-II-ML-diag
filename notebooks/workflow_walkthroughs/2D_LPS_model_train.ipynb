{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload when refreshing notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# import Python functions \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from Python_Functions.functions import cropProfmonImg, matstruct_to_dict, extractDAQBSAScalars, segment_centroids_and_com, plot2DbunchseparationVsCollimatorAndBLEN, extract_processed_images, apply_centroid_correction,apply_tcav_zeroing_filter, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat \n",
    "import re\n",
    "import os \n",
    "# Assumed: commonIndexFromSteps, extractDAQBSAScalars, and other helper functions are available\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Define the list of (experiment, runname, step_identifier) pairs to load\n",
    "# ----------------------------------------------------------------------\n",
    "run_pairs = [\n",
    "    ('E338', '12710', 1),  # Example pairs, modify this list\n",
    "    #('E300', '12431', 1),\n",
    "    # Add more pairs here...\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Initialize lists for concatenation\n",
    "# ----------------------------------------------------------------------\n",
    "all_images = []\n",
    "all_predictors = []\n",
    "all_indices = []\n",
    "\n",
    "print(\"Starting multi-run data loading and concatenation...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Loop through runs, load data, and concatenate\n",
    "# ----------------------------------------------------------------------\n",
    "for experiment, runname, step_id in run_pairs:\n",
    "    \n",
    "    # --- A. Load Processed LPSImage Data and Good Shots Index ---\n",
    "    pickle_filename = f'../../data/processed/LPSImage_goodshots_{experiment}_{runname}_{step_id}.pkl'\n",
    "\n",
    "    try:\n",
    "        with open(pickle_filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        LPSImage_good = data['LPSImage'] # Filtered LPS images\n",
    "        # This 'goodShots' index is relative to the phase-filtered data (all_idx).\n",
    "        goodShots_scal_common_index = data['scalarCommonIndex'] \n",
    "        \n",
    "        print(f\"Loaded {experiment}_{runname}: LPSImage shape {LPSImage_good.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: Pickle file not found at {pickle_filename}\")\n",
    "        continue\n",
    "    \n",
    "    # --- B. Load and Filter Predictor Data (BSA Scalars) ---\n",
    "    \n",
    "    # 1. Load data_struct\n",
    "    dataloc = f'../../data/raw/{experiment}/{experiment}_{runname}/{experiment}_{runname}.mat'\n",
    "    try:\n",
    "        mat = loadmat(dataloc,struct_as_record=False, squeeze_me=True)\n",
    "        data_struct = mat['data_struct']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: .mat file not found at {dataloc}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Extract full BSA scalars (filtered by step_list if needed)\n",
    "    # Don't filter by common index here, we'll do it with the goodShots scalar common index loaded from the file\n",
    "    bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct, filter_index=False)\n",
    "    bsaScalarData = apply_tcav_zeroing_filter(bsaScalarData, bsaVars)\n",
    "\n",
    "    # 3. \n",
    "\n",
    "    # 5. Filter BSA data using the final index\n",
    "    # goodShots_scal_common_index is 1 based indexing from MATLAB, convert to 0 based\n",
    "    bsaScalarData_filtered = bsaScalarData[:, goodShots_scal_common_index - 1]\n",
    "    \n",
    "    # 6. Construct the predictor array\n",
    "    predictor_current = np.vstack(bsaScalarData_filtered).T\n",
    "    \n",
    "    # C. Append to master lists\n",
    "    all_images.append(LPSImage_good)\n",
    "    all_predictors.append(predictor_current)\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Concatenate and finalize arrays\n",
    "# ----------------------------------------------------------------------\n",
    "# Combine all data arrays from the runs\n",
    "images = np.concatenate(all_images, axis=0)\n",
    "predictor = np.concatenate(all_predictors, axis=0)\n",
    "\n",
    "# Set image half dimensions (should match preprocessing)\n",
    "yrange = 100\n",
    "xrange = 100\n",
    "\n",
    "print(\"\\n--- Final Concatenated Data Shapes ---\")\n",
    "print(f\"Total LPS Images (images): {images.shape}\")\n",
    "print(f\"Total Predictors (predictor): {predictor.shape}\")\n",
    "\n",
    "\n",
    "# --- Original scaling and splitting logic follows ---\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "iz_scaler = MinMaxScaler()\n",
    "x_scaled = x_scaler.fit_transform(predictor)\n",
    "Iz_scaled = iz_scaler.fit_transform(images)\n",
    "\n",
    "# 80/20 train-test split\n",
    "x_train_full, x_test_scaled, Iz_train_full, Iz_test_scaled, ntrain, ntest = train_test_split(\n",
    "    x_scaled, Iz_scaled, np.arange(images.shape[0]), test_size=0.2, random_state = 42)\n",
    "\n",
    "# 20% validation split \n",
    "x_train_scaled, x_validation, Iz_train_scaled, y_validation = train_test_split(\n",
    "    x_train_full, Iz_train_full, test_size=0.2, random_state = 42)\n",
    "\n",
    "# compress pixels \n",
    "pca = PCA(n_components=100)\n",
    "compressed_targets = pca.fit_transform(Iz_train_scaled) \n",
    "print(Iz_train_scaled.shape, compressed_targets.shape)\n",
    "y_validation = pca.transform(y_validation)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_validation = torch.tensor(x_validation, dtype=torch.float32)\n",
    "X_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "Y_train = torch.tensor(compressed_targets, dtype=torch.float32)\n",
    "y_validation = torch.tensor(y_validation, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Iz_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(X_train, Y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = MLP(X_train.shape[1], Y_train.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Define custom weighted MSE loss function \n",
    "def custom_loss(y_pred,y_true): \n",
    "    ########################################################\n",
    "    # Version 1: Weighted MSE to emphasize edges\n",
    "    mse = (y_true - y_pred)**2\n",
    "    weights = 1 + 0.7*((y_true < 0.2)|(y_true > 0.8)).float()\n",
    "    return torch.mean(weights*mse)\n",
    "    ########################################################\n",
    "    # Version 2: Logarithmic MSE to emphasize small values\n",
    "    # log_true = torch.log(y_true + 1)\n",
    "    # log_pred = torch.log(y_pred + 1)\n",
    "    \n",
    "    # return torch.mean((log_true - log_pred)**2)\n",
    "    ########################################################\n",
    "\n",
    "# Training loop \n",
    "n_epochs = 200\n",
    "patience = 25\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Fit the nn model on the training set\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = custom_loss(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_dl)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(x_validation)\n",
    "        val_loss = custom_loss(val_pred, y_validation).item()\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "    \n",
    "model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "time_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "joblib_file = '../../model/LPS/MLP_LPS_'+experiment+'_'+runname+'_'+time_stamp+'.pkl'  \n",
    "joblib.dump(model, joblib_file)\n",
    "pca_file = '../../model/LPS/' + experiment + '_' + runname + '_pca_'+time_stamp+'.pkl'\n",
    "pickle.dump(pca, open(pca_file,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_train_scaled = model(X_train).numpy()\n",
    "    pred_test_scaled = model(X_test).numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "pred_train_full = iz_scaler.inverse_transform(pca.inverse_transform(pred_train_scaled))\n",
    "pred_test_full = iz_scaler.inverse_transform(pca.inverse_transform(pred_test_scaled))\n",
    "Iz_train_true = iz_scaler.inverse_transform(Iz_train_scaled)\n",
    "Iz_test_true = iz_scaler.inverse_transform(Iz_test_scaled)\n",
    "elapsed = time.time() - t0\n",
    "print(\"Elapsed time [mins] = {:.1f} \".format(elapsed/60))\n",
    "\n",
    "# Compute R² score\n",
    "def r2_score(true, pred):\n",
    "    RSS = np.sum((true - pred)**2)\n",
    "    TSS = np.sum((true - np.mean(true))**2)\n",
    "    return 1 - RSS / TSS if TSS != 0 else s0\n",
    "\n",
    "print(\"Train R²: {:.2f} %\".format(r2_score(Iz_train_true.ravel(), pred_train_full.ravel()) * 100))\n",
    "print(\"Test R²: {:.2f} %\".format(r2_score(Iz_test_true.ravel(), pred_test_full.ravel()) * 100))\n",
    "\n",
    "# Plot histogram of R² values for each test sample\n",
    "r2_values = [r2_score(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,i], pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,i]) for i in range(Iz_test_true.shape[0])]\n",
    "# Throw away values outside 0 to 1, and count the number of throws\n",
    "r2_values_new = [r2 for r2 in r2_values if 0 <= r2 <= 1]\n",
    "num_throws = len(r2_values) - len(r2_values_new)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(r2_values_new, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of R² Values for Test Samples')\n",
    "plt.xlabel('R² Value')\n",
    "plt.ylabel(f'Plotted Samples: {len(r2_values) - num_throws} / Total Samples: {len(r2_values)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot true vs prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "def plot_xtcav_image_pred(idx):\n",
    "    fig, (ax1, ax2, cx1) = plt.subplots(1,3,figsize=(10, 3), gridspec_kw={'width_ratios': [1, 1, 0.02]})\n",
    "    true_im = np.flip(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0)\n",
    "    im1 = ax1.imshow(true_im, cmap = \"jet\",aspect='auto', vmin = 0, vmax = 400)\n",
    "\n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = f\"True(Shot Number: {ntest[idx]})\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "    pred_im = np.flip(pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0)\n",
    "    im2 = ax2.imshow(pred_im, cmap = \"jet\",aspect='auto',vmin = 0, vmax = 400)\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "    cbar = fig.colorbar(im1, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(true_im, pred_im)\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of PV Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bsaScalarData TCAV_LI20_2400_P and TCAV_LI20_2400_A, most important predictors.\n",
    "bsaVarNames = bsaVars\n",
    "var1_name = 'TCAV_LI20_2400_P'\n",
    "var2_name = 'TCAV_LI20_2400_A'\n",
    "var1_idx = bsaVarNames.index(var1_name)\n",
    "var2_idx = bsaVarNames.index(var2_name)\n",
    "print(f\"Plotting BSA Scalars: {var1_name} (index {var1_idx}) and {var2_name} (index {var2_idx})\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(predictor[:, var1_idx], predictor[:, var2_idx], alpha=0.5)\n",
    "plt.xlabel(var1_name)\n",
    "plt.ylabel(var2_name)\n",
    "plt.title('BSA Scalar Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming bsaScalarData has shape (N_variables, N_samples) from your function\n",
    "# Transpose the data so features are columns and samples are rows for scikit-learn PCA\n",
    "X = bsaScalarData.T \n",
    "\n",
    "# Apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Determine the maximum number of components to test\n",
    "# X.shape is (N_samples, N_variables). max_components = min(N_samples, N_variables) - 1 for a stable PCA\n",
    "max_components = min(X.shape) // 5 \n",
    "\n",
    "# Lists to store results\n",
    "n_components_list = []\n",
    "reconstruction_losses = []\n",
    "\n",
    "# Loop through possible number of components\n",
    "for k in range(1, max_components + 1):\n",
    "    # 1. Initialize and fit PCA\n",
    "    pca_study = PCA(n_components=k)\n",
    "    pca_study.fit(X)\n",
    "    \n",
    "    # 2. Transform and Inverse Transform (Reconstruct)\n",
    "    X_reduced = pca_study.transform(X)\n",
    "    X_reconstructed = pca_study.inverse_transform(X_reduced)\n",
    "    \n",
    "    # 3. Calculate Reconstruction Loss (Mean Squared Error)\n",
    "    loss = mean_squared_error(X, X_reconstructed)\n",
    "    \n",
    "    # Store results\n",
    "    n_components_list.append(k)\n",
    "    reconstruction_losses.append(loss)\n",
    "    \n",
    "# 4. Plot the Results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(n_components_list, reconstruction_losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Reconstruction Loss (Mean Squared Error)')\n",
    "# Log scale in y-axis for better visualization\n",
    "plt.yscale('log')\n",
    "plt.title('PCA of BSA Scalars: Reconstruction Loss vs. Number of Components')\n",
    "plt.grid(True)\n",
    "# 5. Identify the \"Elbow\" point visually after plotting plt.show()\n",
    "plt.show() \n",
    "\n",
    "# After plotting, the optimal number of components is the 'elbow' point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_components = 11\n",
    "\n",
    "pca_comp_study = PCA(n_components=num_components)\n",
    "pca_comp_study.fit(X)\n",
    "# 1. Get explained variance and component loadings\n",
    "\n",
    "variance_ratios = pca_comp_study.explained_variance_ratio_\n",
    "loadings = pca_comp_study.components_\n",
    "\n",
    "# 2. Create the DataFrame for component composition\n",
    "# Each row in 'loadings' is a principal component (PC)\n",
    "# Each column corresponds to a feature (PV)\n",
    "df_loadings = pd.DataFrame(loadings, columns=bsaVars)\n",
    "\n",
    "# 3. Add Component labels and Significance\n",
    "component_labels = [f'PC {i+1}' for i in range(num_components)]\n",
    "df_loadings.insert(0, 'Component', component_labels)\n",
    "df_loadings.insert(1, 'Significance (Explained Variance Ratio)', variance_ratios)\n",
    "\n",
    "# 4. Format the output\n",
    "# The components are already ordered by significance (PC 1 is most significant)\n",
    "# Format the significance column as a percentage for clarity\n",
    "df_loadings['Significance (Explained Variance Ratio)'] = \\\n",
    "    df_loadings['Significance (Explained Variance Ratio)'].map(lambda x: f'{x:.4f} ({x*100:.2f}%)')\n",
    "\n",
    "# Format the loadings to a fixed number of decimal places\n",
    "for col in bsaVars:\n",
    "    df_loadings[col] = df_loadings[col].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "composition_table = df_loadings.to_markdown(index=False)\n",
    "print(\"Composition of Principal Components:\")\n",
    "print(composition_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract and Square the Loadings (Loadings matrix is: Components x Variables)\n",
    "# loadings = pca_comp_study.components_ (from your provided code block)\n",
    "squared_loadings = loadings**2\n",
    "\n",
    "# 2. Sum Across Components (Sum columns to get total significance per variable)\n",
    "# The result is an array where each element is the total squared loading for a variable\n",
    "total_squared_loadings = np.sum(squared_loadings, axis=0)\n",
    "\n",
    "# 3. Create a DataFrame for sorting\n",
    "# bsaVars is the list of variable names (features)\n",
    "df_var_significance = pd.DataFrame({\n",
    "    'BSA Variable (PV)': bsaVars,\n",
    "    'Total Squared Loading (Significance)': total_squared_loadings\n",
    "})\n",
    "\n",
    "# 4. Sort in descending order\n",
    "df_var_significance = df_var_significance.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 5. Specify the number of top variables to display\n",
    "N_top = 20  # Example: display the top few most significant variables\n",
    "\n",
    "# 6. Format and display the table\n",
    "df_top_vars = df_var_significance.head(N_top)\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "print(f\"Top {N_top} BSA Variables Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_var_significance[\n",
    "    ~df_var_significance['BSA Variable (PV)'].str.contains('BPM', case=False, na=False)\n",
    "]\n",
    "\n",
    "# 5. Sort the filtered variables in descending order of significance\n",
    "df_filtered = df_filtered.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 6. Select Top N and format\n",
    "N_top = 20 # Display the top few most significant non-BPM variables\n",
    "df_top_vars = df_filtered.head(N_top)\n",
    "\n",
    "# Format the significance column\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "\n",
    "print(f\"Top {N_top} BSA Variables (EXCLUDING BPMS) Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Layout\n",
    "import torch\n",
    "\n",
    "index_to_examine = 10  # Change this index to examine different test samples\n",
    "\n",
    "# Variables to be controlled (Non-BPM variables from previous ranking logic)\n",
    "CONTROL_VARS = [\n",
    "    'BLEN_LI11_359_BRAW', 'TCAV_LI20_2400_A', 'TCAV_LI20_2400_P', \n",
    "    'LASR_LT10_930_PWR', 'PMTR_HT10_950_PWR', 'PMT_LI20_3350_QDCRAW', \n",
    "    'KLYS_LI10_41_FB_FAST_PACT', 'KLYS_LI10_41_FB_FAST_AACT'\n",
    "]\n",
    "# ==============================================================================\n",
    "# --- DYNAMIC PREDICTION FUNCTION ---\n",
    "# ==============================================================================\n",
    "original_predictor_reconstructed = x_scaler.inverse_transform(X_test.numpy())\n",
    "def plot_dynamic_pred(**slider_values):\n",
    "    idx = index_to_examine\n",
    "    fig, (ax1, ax2,ax3, cx1) = plt.subplots(1,4,figsize=(10, 3), gridspec_kw={'width_ratios': [1, 1,1, 0.02]})\n",
    "    im1 = ax1.imshow(np.flip(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0), cmap = \"jet\",aspect='auto', vmin = 0, vmax = 400)\n",
    "   \n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = f\"True(Shot Number: {ntest[idx]})\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "    im2 = ax2.imshow(np.flip(pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0), cmap = \"jet\",aspect='auto',vmin = 0, vmax = 400)\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "\n",
    "    # Modify the predictor for the given index based on slider values\n",
    "    modified_predictor = original_predictor_reconstructed[idx].copy()\n",
    "    for var_name, slider in slider_values.items():\n",
    "        pv_index = bsaVarNames.index(var_name)\n",
    "        modified_predictor[pv_index] = slider\n",
    "\n",
    "    # Scale back the modified predictor\n",
    "    modified_predictor_scaled = x_scaler.transform(modified_predictor.reshape(1, -1))\n",
    "\n",
    "    # Make prediction with modified predictor\n",
    "    modified_predictor_tensor = torch.tensor(modified_predictor_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        modified_pred_scaled = model(modified_predictor_tensor).numpy()\n",
    "    modified_pred_full = iz_scaler.inverse_transform(pca.inverse_transform(modified_pred_scaled))\n",
    "    im3 = ax3.imshow(np.flip(modified_pred_full.T.reshape(2*yrange,2*xrange)[:,:], axis=0), cmap = \"jet\",aspect='auto',vmin = 0, vmax = 400)\n",
    "    ax3.set(xlabel = \"Time [fs]\")\n",
    "    ax3.set(ylabel = \"y [pix]\")\n",
    "    ax3.set(title = \"Modified Prediction\")\n",
    "    ax3.set(xlim = (0,2*xrange))\n",
    "    ax3.set(ylim= (0,2*yrange))\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    r2_val = r2_score(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx])\n",
    "    plt.suptitle(f'R² Value: {r2_val:.4f}', fontsize=7)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "raw_min_max = {}\n",
    "for i, pv_name in enumerate(CONTROL_VARS):\n",
    "    pv_index = bsaVarNames.index(pv_name)\n",
    "    raw_min_max[pv_name] = (predictor[:, pv_index].min(), predictor[:, pv_index].max())\n",
    "\n",
    "N_SAMPLES = Iz_test_true.shape[0]\n",
    "\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "for i, pv_name in enumerate(CONTROL_VARS):\n",
    "    min_val, max_val = raw_min_max[pv_name]\n",
    "    pv_index = bsaVarNames.index(pv_name)\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    initial_value = original_predictor_reconstructed[index_to_examine, pv_index] # Use the value for the initial index (idx=0)\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value,\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_dynamic_pred, **widget_kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtcav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
