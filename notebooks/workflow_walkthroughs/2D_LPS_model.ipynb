{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload when refreshing notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# import Python functions \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from Python_Functions.functions import cropProfmonImg, matstruct_to_dict, extractDAQBSAScalars, segment_centroids_and_com, plot2DbunchseparationVsCollimatorAndBLEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XTCAV calibration\n",
    "krf = 239.26\n",
    "cal = 1167 # um/deg  http://physics-elog.slac.stanford.edu/facetelog/show.jsp?dir=/2025/11/13.03&pos=2025-$\n",
    "streakFromGUI = cal*krf*180/np.pi*1e-6#um/um\n",
    "\n",
    "# Sets the main beam energy\n",
    "mainbeamE_eV = 10e9\n",
    "# Sets the dnom value for CHER\n",
    "dnom = 59.8e-3\n",
    "\n",
    "# Sets data location\n",
    "experiment = 'E300'\n",
    "runname = '12431'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataset\n",
    "dataloc = '../../data/raw/' + experiment + '/' + experiment + '_' + runname + '/' + experiment + '_'  +runname + '.mat'\n",
    "mat = loadmat(dataloc,struct_as_record=False, squeeze_me=True)\n",
    "data_struct = mat['data_struct']\n",
    "\n",
    "# Extracts number of steps\n",
    "stepsAll = data_struct.params.stepsAll\n",
    "if stepsAll is None or len(np.atleast_1d(stepsAll)) == 0:\n",
    "    stepsAll = [1]\n",
    "\n",
    "# calculate xt calibration factor\n",
    "xtcalibrationfactor = data_struct.metadata.DTOTR2.RESOLUTION*1e-6/streakFromGUI/3e8\n",
    "\n",
    "# cropping aspect ratio \n",
    "xrange = 100 \n",
    "yrange = xrange\n",
    "\n",
    "\n",
    "# gaussian filter parameter\n",
    "hotPixThreshold = 1e3\n",
    "sigma = 1\n",
    "threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current profiles and 2D LPS images \n",
    "xtcavImages_list = []\n",
    "xtcavImages_list_raw = []\n",
    "horz_proj_list = []\n",
    "LPSImage = [] \n",
    "\n",
    "for a in range(len(stepsAll)):\n",
    "    if len(stepsAll) == 1:\n",
    "        raw_path = data_struct.images.DTOTR2.loc\n",
    "    else: \n",
    "        raw_path = data_struct.images.DTOTR2.loc[a]\n",
    "    match = re.search(rf'({experiment}_\\d+/images/DTOTR2/DTOTR2_data_step\\d+\\.h5)', raw_path)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Path format invalid or not matched: {raw_path}\")\n",
    "\n",
    "    DTOTR2datalocation = '../../data/raw/'+ experiment + '/' + match.group(0)\n",
    "\n",
    "    with h5py.File(DTOTR2datalocation, 'r') as f:\n",
    "        data_raw = f['entry']['data']['data'][:].astype(np.float64)  # shape: (N, H, W)\n",
    "    \n",
    "    # Transpose to shape: (H, W, N)\n",
    "    DTOTR2data_step = np.transpose(data_raw, (2, 1, 0))\n",
    "    xtcavImages_step = DTOTR2data_step - data_struct.backgrounds.DTOTR2[:,:,np.newaxis].astype(np.float64)\n",
    "    \n",
    "    for idx in range(DTOTR2data_step.shape[2]):\n",
    "        if idx is None:\n",
    "            continue\n",
    "        image = xtcavImages_step[:,:,idx]\n",
    "        xtcavImages_list_raw.append(image[:,:,np.newaxis])\n",
    "        \n",
    "        # crop images \n",
    "        image_cropped, _ = cropProfmonImg(image, xrange, yrange, plot_flag=False)\n",
    "        img_filtered = median_filter(image_cropped, size=3)\n",
    "        hotPixels = img_filtered > hotPixThreshold\n",
    "        img_filtered = np.ma.masked_array(img_filtered, hotPixels)\n",
    "        processed_image = gaussian_filter(img_filtered, sigma=sigma, radius = 6*sigma + 1)\n",
    "        processed_image[processed_image < threshold] = 0.0\n",
    "        Nrows = np.array(processed_image).shape[0]\n",
    "        \n",
    "        # calcualte current profiles \n",
    "        horz_proj_idx = np.sum(processed_image, axis=0)\n",
    "        horz_proj_idx = horz_proj_idx[:,np.newaxis]\n",
    "        processed_image = processed_image[:,:,np.newaxis]\n",
    "        image_ravel = processed_image.ravel()\n",
    "        # combine current profiles into one array \n",
    "        horz_proj_list.append(horz_proj_idx)\n",
    "\n",
    "        # combine images into one array \n",
    "        xtcavImages_list.append(processed_image)\n",
    "        LPSImage.append([image_ravel])\n",
    "\n",
    "xtcavImages = np.concatenate(xtcavImages_list, axis=2)\n",
    "xtcavImages_raw = np.concatenate(xtcavImages_list_raw, axis=2)\n",
    "horz_proj = np.concatenate(horz_proj_list, axis=1)\n",
    "LPSImage = np.concatenate(LPSImage, axis = 0)\n",
    "\n",
    "# Keeps only the data with a common index\n",
    "DTOTR2commonind = data_struct.images.DTOTR2.common_index -1 \n",
    "horz_proj = horz_proj[:,DTOTR2commonind]\n",
    "xtcavImages = xtcavImages[:,:,DTOTR2commonind]\n",
    "xtcavImages_raw = xtcavImages_raw[:,:,DTOTR2commonind]\n",
    "\n",
    "#Make a copy of xtcavImages for comparison of before and after centroid correction\n",
    "xtcavImages_centroid_uncorrected = xtcavImages.copy()\n",
    "LPSImage = LPSImage[DTOTR2commonind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct)\n",
    "\n",
    "ampl_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_A' in var)\n",
    "xtcavAmpl = bsaScalarData[ampl_idx, :]\n",
    "\n",
    "phase_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_P' in var)\n",
    "xtcavPhase = bsaScalarData[phase_idx, :]\n",
    "\n",
    "xtcavOffShots = xtcavAmpl<0.1\n",
    "xtcavPhase[xtcavOffShots] = 0 #Set this for ease of plotting\n",
    "\n",
    "isChargePV = [bool(re.search(r'TORO_LI20_2452_TMIT', pv)) for pv in bsaVars]\n",
    "pvidx = [i for i, val in enumerate(isChargePV) if val]\n",
    "charge = bsaScalarData[pvidx, :] * 1.6e-19  # in C \n",
    "\n",
    "minus_90_idx = np.where((xtcavPhase >= -91) & (xtcavPhase <= -89))[0]\n",
    "plus_90_idx = np.where((xtcavPhase >= 89) & (xtcavPhase <= 91))[0]\n",
    "off_idx = np.where(xtcavPhase == 0)[0]\n",
    "all_idx = np.append(minus_90_idx,plus_90_idx)\n",
    "\n",
    "currentProfile_all = [] \n",
    "\n",
    "# Process all degree shots\n",
    "for ij in range(len(all_idx)):\n",
    "    idx = all_idx[ij]\n",
    "    streakedProfile = horz_proj[:,idx]\n",
    "\n",
    "    tvar = np.arange(1, len(streakedProfile) + 1) * xtcalibrationfactor\n",
    "    tvar = tvar - np.median(tvar)  # Center around zero\n",
    "\n",
    "    prefactor = charge[0, idx] / np.trapz(streakedProfile, tvar)\n",
    "\n",
    "    currentProfile = 1e-3 * streakedProfile * prefactor  # Convert to kA\n",
    "    currentProfile_all.append(currentProfile)\n",
    "    \n",
    "currentProfile_all = np.array(currentProfile_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Correction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bsaVars)\n",
    "# BPMS_LI20_2445_X is supposed to measure the beam energy right before the TCAV\n",
    "# BPMS_LI14_801_X is supposed to measure the beam energy at LI14\n",
    "# 'BLEN_LI14_888_BRAW' is the length of the bunch at LI14\n",
    "# 'BLEN_LI11_359_BRAW' is the length of the bunch at LI11\n",
    "# 'BPMS_LI11_333_X' is supposed to measure the beam energy at LI11\n",
    "energy_idx = next(i for i, var in enumerate(bsaVars) if 'BPMS_LI11_333_X' in var)\n",
    "beamEnergyM = bsaScalarData[energy_idx, minus_90_idx]\n",
    "beamEnergyP = bsaScalarData[energy_idx, plus_90_idx]\n",
    "beamEnergyO = bsaScalarData[energy_idx, off_idx]\n",
    "# print(beamEnergy)\n",
    "# Create LPS image center of mass y coordinate vs beam energy plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(beamEnergyM, np.array([np.sum(xtcavImages[:,:,i]*np.arange(xtcavImages.shape[1])[np.newaxis,:])/np.sum(xtcavImages[:,:,i]) for i in minus_90_idx]), c='blue', s=5)\n",
    "plt.scatter(beamEnergyP, np.array([np.sum(xtcavImages[:,:,i]*np.arange(xtcavImages.shape[1])[np.newaxis,:])/np.sum(xtcavImages[:,:,i]) for i in plus_90_idx]), c='red', s=5)\n",
    "plt.scatter(beamEnergyO, np.array([np.sum(xtcavImages[:,:,i]*np.arange(xtcavImages.shape[1])[np.newaxis,:])/np.sum(xtcavImages[:,:,i]) for i in off_idx]), c='green', s=5)\n",
    "\n",
    "plt.xlabel('BPMS_LI11_333_X')\n",
    "plt.ylabel('LPS Image Center of Mass [pix]')\n",
    "plt.title('LPS Image Center of Mass vs Beam Energy PV, Cropped Image')\n",
    "plt.legend(['-90 deg','+90 deg','0 deg'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(beamEnergyM, np.array([np.sum(xtcavImages_raw[:,:,i]*np.arange(xtcavImages_raw.shape[1])[np.newaxis,:])/np.sum(xtcavImages_raw[:,:,i]) for i in minus_90_idx]), c='blue', s=5)\n",
    "plt.scatter(beamEnergyP, np.array([np.sum(xtcavImages_raw[:,:,i]*np.arange(xtcavImages_raw.shape[1])[np.newaxis,:])/np.sum(xtcavImages_raw[:,:,i]) for i in plus_90_idx]), c='red', s=5)\n",
    "plt.scatter(beamEnergyO, np.array([np.sum(xtcavImages_raw[:,:,i]*np.arange(xtcavImages_raw.shape[1])[np.newaxis,:])/np.sum(xtcavImages_raw[:,:,i]) for i in off_idx]), c='green', s=5)\n",
    "\n",
    "plt.xlabel('BPMS_LI11_333_X')\n",
    "plt.ylabel('LPS Image Center of Mass [pix]')\n",
    "plt.title('LPS Image Center of Mass vs Beam Energy PV, RAW Image')\n",
    "plt.legend(['-90 deg','+90 deg','0 deg'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Centroid Correction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def construct_centroid_function(images, off_idx, smoothing_window_size=5, max_degree=1):\n",
    "    \"\"\"\n",
    "    Constructs a smoothed centroid function with localized quadratic extrapolation.\n",
    "\n",
    "    This function calculates the mean horizontal center of mass (COM) for each\n",
    "    row across a selection of images. It handles unreliable data with advanced logic:\n",
    "    1.  **Interpolation**: Fills gaps for rows with high COM variance using linear\n",
    "        interpolation between stable rows.\n",
    "    2.  **Smoothing**: Applies a moving average filter to the entire COM profile.\n",
    "    3.  **Local Extrapolation**: For rows far from any stable data, it performs\n",
    "        two separate polynomial fits:\n",
    "        -   **Top Extrapolation**: Fits a degree-2 polynomial to the top-most\n",
    "            stable rows (up to 7 points) to project the trend upwards.\n",
    "        -   **Bottom Extrapolation**: Fits a separate degree-2 polynomial to the\n",
    "            bottom-most stable rows to project the trend downwards.\n",
    "        This local approach correctly handles non-uniform trends (e.g., S-curves).\n",
    "\n",
    "    Args:\n",
    "        images (list or np.ndarray): A list or 3D NumPy array of 2D image arrays.\n",
    "                                     All images must have the same dimensions.\n",
    "        off_idx (list or np.ndarray): Indices of images to use for the calculation.\n",
    "        smoothing_window_size (int, optional): The size of the moving average window\n",
    "                                               for smoothing. Must be an odd number.\n",
    "                                               Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D array where each value is the integer horizontal shift\n",
    "                    required to center the content of that row.\n",
    "    \"\"\"\n",
    "    if not isinstance(off_idx, (list, np.ndarray)):\n",
    "        raise ValueError(\"off_idx must be a non-empty list or array of indices.\")\n",
    "    if smoothing_window_size % 2 != 1:\n",
    "        raise ValueError(\"smoothing_window_size must be a positive odd number.\")\n",
    "\n",
    "    # 1. Select images and get dimensions\n",
    "    selected_images = np.array([images[:,:,i] for i in off_idx])\n",
    "    num_images, num_rows, num_cols = selected_images.shape\n",
    "    image_center = num_cols / 2.0\n",
    "\n",
    "    # 2. Calculate Center of Mass (COM) for each row\n",
    "    col_indices = np.arange(num_cols)\n",
    "    epsilon = 1e-9\n",
    "    row_sums = selected_images.sum(axis=2)\n",
    "    all_row_coms = np.sum(selected_images * col_indices, axis=2) / (row_sums + epsilon)\n",
    "    all_row_coms[row_sums == 0] = np.nan\n",
    "\n",
    "    # 3. Identify stable (\"good\") rows\n",
    "    mean_coms = np.nanmean(all_row_coms, axis=0)\n",
    "    std_dev_coms = np.nanstd(all_row_coms, axis=0)\n",
    "    variance_threshold = 0.15 * num_cols\n",
    "    good_rows_indices = np.where(std_dev_coms <= variance_threshold)[0]\n",
    "    all_row_indices = np.arange(num_rows)\n",
    "    \n",
    "    # Handle cases with insufficient good data\n",
    "    if len(good_rows_indices) < 2:\n",
    "        print(\"Warning: Fewer than 2 stable rows found. Cannot perform reliable analysis.\")\n",
    "        return np.zeros(num_rows, dtype=int)\n",
    "        \n",
    "    # 4. Interpolate and Smooth\n",
    "    good_com_values = mean_coms[good_rows_indices]\n",
    "    interpolated_coms = np.interp(all_row_indices, good_rows_indices, good_com_values)\n",
    "    \n",
    "    if smoothing_window_size > 1:\n",
    "        kernel = np.ones(smoothing_window_size) / smoothing_window_size\n",
    "        smoothed_coms = np.convolve(interpolated_coms, kernel, mode='same')\n",
    "    else:\n",
    "        smoothed_coms = interpolated_coms\n",
    "        \n",
    "    # 5. Perform LOCAL EXTRAPOLATION\n",
    "    final_coms = np.copy(smoothed_coms) # Start with interpolated/smoothed data\n",
    "    min_good_idx, max_good_idx = good_rows_indices[0], good_rows_indices[-1]\n",
    "\n",
    "    # --- Top Extrapolation ---\n",
    "    top_extrap_indices = np.arange(0, min_good_idx)\n",
    "    if top_extrap_indices.size > 0:\n",
    "        # Select up to degree+5 (7) points from the top of the stable region\n",
    "        fit_indices = good_rows_indices[:max_degree + 5]\n",
    "        fit_values = good_com_values[:max_degree + 5]\n",
    "        \n",
    "        # Need at least degree+1 points to fit. We require 2 for linear, 3 for quadratic.\n",
    "        if len(fit_indices) >= 2:\n",
    "            degree = min(max_degree, len(fit_indices) - 1)\n",
    "            coeffs = np.polyfit(fit_indices, fit_values, degree)\n",
    "            poly_func = np.poly1d(coeffs)\n",
    "            final_coms[top_extrap_indices] = poly_func(top_extrap_indices)\n",
    "            \n",
    "    # --- Bottom Extrapolation ---\n",
    "    bottom_extrap_indices = np.arange(max_good_idx + 1, num_rows)\n",
    "    if bottom_extrap_indices.size > 0:\n",
    "        # Select up to degree+5 (7) points from the bottom of the stable region\n",
    "        fit_indices = good_rows_indices[-(max_degree + 5):]\n",
    "        fit_values = good_com_values[-(max_degree + 5):]\n",
    "\n",
    "        if len(fit_indices) >= 2:\n",
    "            degree = min(max_degree, len(fit_indices) - 1)\n",
    "            coeffs = np.polyfit(fit_indices, fit_values, degree)\n",
    "            poly_func = np.poly1d(coeffs)\n",
    "            final_coms[bottom_extrap_indices] = poly_func(bottom_extrap_indices)\n",
    "\n",
    "    # 6. Calculate the final correction shift\n",
    "    horizontal_correction = image_center - final_coms\n",
    "    return np.round(horizontal_correction).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_corrections = construct_centroid_function(xtcavImages, off_idx)\n",
    "# Extract current profiles and 2D LPS images \n",
    "xtcavImages_list = []\n",
    "horz_proj_list = []\n",
    "#LPS is overwritten in this optional step. If not needed, this block can be skipped.\n",
    "LPSImage = [] \n",
    "\n",
    "for a in range(len(stepsAll)):\n",
    "    if len(stepsAll) == 1:\n",
    "        raw_path = data_struct.images.DTOTR2.loc\n",
    "    else: \n",
    "        raw_path = data_struct.images.DTOTR2.loc[a]\n",
    "    match = re.search(rf'({experiment}_\\d+/images/DTOTR2/DTOTR2_data_step\\d+\\.h5)', raw_path)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Path format invalid or not matched: {raw_path}\")\n",
    "\n",
    "    DTOTR2datalocation = '../../data/raw/'+ experiment + '/' + match.group(0)\n",
    "\n",
    "    with h5py.File(DTOTR2datalocation, 'r') as f:\n",
    "        data_raw = f['entry']['data']['data'][:].astype(np.float64)  # shape: (N, H, W)\n",
    "    \n",
    "    # Transpose to shape: (H, W, N)\n",
    "    DTOTR2data_step = np.transpose(data_raw, (2, 1, 0))\n",
    "    xtcavImages_step = DTOTR2data_step - data_struct.backgrounds.DTOTR2[:,:,np.newaxis].astype(np.float64)\n",
    "    \n",
    "    for idx in range(DTOTR2data_step.shape[2]):\n",
    "        if idx is None:\n",
    "            continue\n",
    "        image = xtcavImages_step[:,:,idx]\n",
    "        \n",
    "        # crop images \n",
    "        image_cropped, _ = cropProfmonImg(image, xrange, yrange, plot_flag=False)\n",
    "        img_filtered = median_filter(image_cropped, size=3)\n",
    "        hotPixels = img_filtered > hotPixThreshold\n",
    "        img_filtered = np.ma.masked_array(img_filtered, hotPixels)\n",
    "        processed_image = gaussian_filter(img_filtered, sigma=sigma, radius = 6*sigma + 1)\n",
    "        processed_image[processed_image < threshold] = 0.0\n",
    "        Nrows = np.array(processed_image).shape[0]\n",
    "\n",
    "        # Apply centroid correction\n",
    "        corrected_image = np.zeros_like(processed_image)\n",
    "        for row in range(Nrows):\n",
    "            shift = centroid_corrections[row]\n",
    "            corrected_image[row, :] = np.roll(processed_image[row, :], shift)\n",
    "        \n",
    "        # calcualte current profiles \n",
    "        horz_proj_idx = np.sum(corrected_image, axis=0)\n",
    "        horz_proj_idx = horz_proj_idx[:,np.newaxis]\n",
    "        corrected_image = corrected_image[:,:,np.newaxis]\n",
    "        image_ravel = corrected_image.ravel()\n",
    "        # combine current profiles into one array \n",
    "        horz_proj_list.append(horz_proj_idx)\n",
    "\n",
    "        # combine images into one array \n",
    "        xtcavImages_list.append(corrected_image)\n",
    "        LPSImage.append([image_ravel])\n",
    "\n",
    "xtcavImages = np.concatenate(xtcavImages_list, axis=2)\n",
    "horz_proj = np.concatenate(horz_proj_list, axis=1)\n",
    "LPSImage = np.concatenate(LPSImage, axis = 0)\n",
    "\n",
    "# Keeps only the data with a common index\n",
    "DTOTR2commonind = data_struct.images.DTOTR2.common_index -1 \n",
    "horz_proj = horz_proj[:,DTOTR2commonind]\n",
    "xtcavImages = xtcavImages[:,:,DTOTR2commonind]\n",
    "LPSImage = LPSImage[DTOTR2commonind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct)\n",
    "\n",
    "ampl_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_A' in var)\n",
    "xtcavAmpl = bsaScalarData[ampl_idx, :]\n",
    "\n",
    "phase_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_P' in var)\n",
    "xtcavPhase = bsaScalarData[phase_idx, :]\n",
    "\n",
    "xtcavOffShots = xtcavAmpl<0.1\n",
    "xtcavPhase[xtcavOffShots] = 0 #Set this for ease of plotting\n",
    "\n",
    "isChargePV = [bool(re.search(r'TORO_LI20_2452_TMIT', pv)) for pv in bsaVars]\n",
    "pvidx = [i for i, val in enumerate(isChargePV) if val]\n",
    "charge = bsaScalarData[pvidx, :] * 1.6e-19  # in C \n",
    "\n",
    "minus_90_idx = np.where((xtcavPhase >= -91) & (xtcavPhase <= -89))[0]\n",
    "plus_90_idx = np.where((xtcavPhase >= 89) & (xtcavPhase <= 91))[0]\n",
    "off_idx = np.where(xtcavPhase == 0)[0]\n",
    "all_idx = np.append(minus_90_idx,plus_90_idx)\n",
    "\n",
    "currentProfile_all = [] \n",
    "\n",
    "# Process all degree shots\n",
    "for ij in range(len(all_idx)):\n",
    "    idx = all_idx[ij]\n",
    "    streakedProfile = horz_proj[:,idx]\n",
    "\n",
    "    tvar = np.arange(1, len(streakedProfile) + 1) * xtcalibrationfactor\n",
    "    tvar = tvar - np.median(tvar)  # Center around zero\n",
    "\n",
    "    prefactor = charge[0, idx] / np.trapz(streakedProfile, tvar)\n",
    "\n",
    "    currentProfile = 1e-3 * streakedProfile * prefactor  # Convert to kA\n",
    "    currentProfile_all.append(currentProfile)\n",
    "    \n",
    "currentProfile_all = np.array(currentProfile_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first shot where tcav is at -90, 0 and +90 deg\n",
    "idx = 1\n",
    "near_minus_90_idx = np.where((xtcavPhase >= -90.55) & (xtcavPhase <= -89.55))[0][idx]\n",
    "near_plus_90_idx = np.where((xtcavPhase >= 89.55) & (xtcavPhase <= 90.55))[0][idx]\n",
    "zero_idx = np.where(xtcavPhase == 0)[0][idx]\n",
    "\n",
    "sample_image_indices = [near_minus_90_idx, zero_idx, near_plus_90_idx]\n",
    "plot_titles = ['Tcav phase -90 deg', '0 deg', '+90 deg']\n",
    "\n",
    "\n",
    "# Define the x and yrange for cropping the image; Need to automate this\n",
    "# figure;imagesc(sampleImage)\n",
    "\n",
    "xrange = 100\n",
    "yrange = xrange\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1, 1, 0.1]})\n",
    "fig.suptitle(f'TCAV images RAW DAQ {experiment} - {runname}', fontsize=14)\n",
    "\n",
    "for i, idx in enumerate(sample_image_indices):\n",
    "    if idx is None:\n",
    "        continue\n",
    "\n",
    "    sample_image = xtcavImages_raw[:, :, idx]\n",
    "\n",
    "    axs[i].imshow(sample_image, cmap='jet', aspect='auto')\n",
    "    axs[i].set_title(plot_titles[i])\n",
    "\n",
    "# Colorbar, top right corner, horizontal\n",
    "cbar = fig.colorbar(axs[2].images[0], cax = axs[3], orientation='vertical', fraction=0.05, pad=0.2)\n",
    "cbar.set_label('Intensity [a.u.]')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "fig.suptitle(f'TCAV images before centroid correction DAQ {experiment} - {runname}', fontsize=14)\n",
    "\n",
    "for i, idx in enumerate(sample_image_indices):\n",
    "    if idx is None:\n",
    "        continue\n",
    "\n",
    "    sample_image = xtcavImages_centroid_uncorrected[:, :, idx]\n",
    "    horz_proj = np.sum(sample_image, axis=0)\n",
    "\n",
    "    axs[0, i].imshow(sample_image, cmap='jet', aspect='auto')\n",
    "    axs[0, i].set_title(plot_titles[i])\n",
    "\n",
    "    axs[1, i].plot(horz_proj)\n",
    "    axs[1, i].set_title(\"Horizontal Projection\")\n",
    "    #If i==1, the center plot, also plot centroid_corrections on the 2d image\n",
    "    if i==1:\n",
    "        for row in range(sample_image.shape[0]):\n",
    "            shift = centroid_corrections[row]\n",
    "            # Plot a dot at (shift, row)\n",
    "            axs[0, i].plot(xrange - shift, row, 'wo', markersize=1)\n",
    "\n",
    "        # Draw a vertical line at the center of mass x\n",
    "        center_of_mass_x = np.sum(horz_proj * np.arange(horz_proj.shape[0])) / np.sum(horz_proj)\n",
    "        axs[0, i].axvline(center_of_mass_x, color='w', linestyle='--')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "fig.suptitle(f'TCAV images after centroid correction DAQ {experiment} - {runname}', fontsize=14)\n",
    "\n",
    "for i, idx in enumerate(sample_image_indices):\n",
    "    if idx is None:\n",
    "        continue\n",
    "\n",
    "    sample_image = xtcavImages[:, :, idx]\n",
    "    horz_proj = np.sum(sample_image, axis=0)\n",
    "\n",
    "    axs[0, i].imshow(sample_image, cmap='jet', aspect='auto')\n",
    "    axs[0, i].set_title(plot_titles[i])\n",
    "\n",
    "    axs[1, i].plot(horz_proj)\n",
    "    axs[1, i].set_title(\"Horizontal Projection\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Good Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out \"bad\" shots with Bi-Gaussian fit \n",
    "def bi_gaussian(x, A1, mu1, sigma1, A2, mu2, sigma2):\n",
    "    return (A1 * np.exp(-(x - mu1)**2 / (2 * sigma1**2)) +\n",
    "            A2 * np.exp(-(x - mu2)**2 / (2 * sigma2**2)))\n",
    "\n",
    "amp1 = []\n",
    "amp2 = []\n",
    "mu1 = []\n",
    "mu2 = []\n",
    "R_squared = []\n",
    "\n",
    "for ij in range(len(all_idx)):\n",
    "    y = currentProfile_all[ij, :]\n",
    "    x = np.arange(len(y))\n",
    "\n",
    "    # Initial guess: [A1, mu1, sigma1, A2, mu2, sigma2]\n",
    "    if xtcavPhase[all_idx][ij] < 0:\n",
    "        initial_guess = [np.max(y), 100, 4, np.max(y)*0.1, 60 + ij*0.15, 4]\n",
    "    elif xtcavPhase[all_idx][ij] > 0:\n",
    "        initial_guess = [np.max(y), 100, 4, np.max(y)*0.1, 60, 4]\n",
    "    \n",
    "    try:\n",
    "        popt, pcov = curve_fit(bi_gaussian, x, y, p0=initial_guess, maxfev=5000)\n",
    "    except RuntimeError:\n",
    "        amp1.append(np.nan)\n",
    "        R_squared.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Extract parameters\n",
    "    A1, mu1_val, sig1, A2, mu2_val, sig2 = popt\n",
    "    amp1.append(A1)\n",
    "    amp2.append(A2)\n",
    "    mu1.append(mu1_val)\n",
    "    mu2.append(mu2_val)\n",
    "\n",
    "    # Evaluate fit\n",
    "    y_fit = bi_gaussian(x, *popt)\n",
    "    SST = np.sum((y - np.mean(y))**2)\n",
    "    SSR = np.sum((y - y_fit)**2)\n",
    "    R_squared.append(1 - SSR / SST)\n",
    "\n",
    "# Convert results to arrays\n",
    "amp1 = np.array(amp1)\n",
    "R_squared = np.array(R_squared)\n",
    "# set requirements for \"good\" shots. For xtcavPhase>0, we want larger (A1) peak at larger x (mu1).\n",
    "# For xtcavPhase<0, we want larger (A2) peak at smaller x (mu2).\n",
    "goodShots = np.where((R_squared > 0.97) & (amp1 < 50))[0]\n",
    "#goodShots_twobunch_tcav = np.where((R_squared > 0.97) & (amp1 < 50) & ((mu1 > mu2) & (amp1 < amp2)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some good shots xtcavOffShots\n",
    "idx = 5\n",
    "fig, (ax1) = plt.subplots(1,1,figsize=(9, 6))\n",
    "im1 = ax1.imshow(xtcavImages[:,:,minus_90_idx[idx]], cmap = \"jet\",aspect='auto')\n",
    "# ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "cbar1 = plt.colorbar(im1, ax=ax1)\n",
    "cbar1.set_label(\"Charge(a.u.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "index = np.sort(all_idx[goodShots])\n",
    "images = LPSImage[all_idx,:][goodShots,:]\n",
    "steps = data_struct.scalars.steps[DTOTR2commonind]\n",
    "predictor = np.vstack((bsaScalarData[:,goodShots], steps[goodShots])).T\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "iz_scaler = MinMaxScaler()\n",
    "x_scaled = x_scaler.fit_transform(predictor)\n",
    "Iz_scaled = iz_scaler.fit_transform(images)\n",
    "\n",
    "# 80/20 train-test split\n",
    "x_train_full, x_test_scaled, Iz_train_full, Iz_test_scaled, ntrain, ntest = train_test_split(\n",
    "    x_scaled, Iz_scaled, index, test_size=0.2, random_state = 42)\n",
    "\n",
    "# 20% validation split \n",
    "x_train_scaled, X_val, Iz_train_scaled, Y_val = train_test_split(\n",
    "    x_train_full, Iz_train_full, test_size=0.2, random_state = 42)\n",
    "\n",
    "# compress pixels \n",
    "pca = PCA(n_components=100)\n",
    "compressed_targets = pca.fit_transform(Iz_train_scaled) \n",
    "print(Iz_train_scaled.shape, compressed_targets.shape)\n",
    "Y_val = pca.transform(Y_val)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "Y_train = torch.tensor(compressed_targets, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Iz_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(X_train, Y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=24, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define MLP structure\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = MLP(X_train.shape[1], Y_train.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Define custom weighted MSE loss function \n",
    "def custom_loss( y_pred,y_true): \n",
    "    mse = (y_true - y_pred)**2\n",
    "    weights = 1 + 0.7*((y_true < 0.2)|(y_true > 0.8)).float()\n",
    "    return torch.mean(weights*mse)\n",
    "\n",
    "# Training loop \n",
    "n_epochs = 200\n",
    "patience = 25\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Fit the nn model on the training set\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = custom_loss(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_dl)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = custom_loss(val_pred, Y_val).item()\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "    \n",
    "model.load_state_dict(best_model_state)\n",
    "    \n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_train_scaled = model(X_train).numpy()\n",
    "    pred_test_scaled = model(X_test).numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "pred_train_full = iz_scaler.inverse_transform(pca.inverse_transform(pred_train_scaled))\n",
    "pred_test_full = iz_scaler.inverse_transform(pca.inverse_transform(pred_test_scaled))\n",
    "Iz_train_true = iz_scaler.inverse_transform(Iz_train_scaled)\n",
    "Iz_test_true = iz_scaler.inverse_transform(Iz_test_scaled)\n",
    "elapsed = time.time() - t0\n",
    "print(\"Elapsed time [mins] = {:.1f} \".format(elapsed/60))\n",
    "\n",
    "# Compute R²\n",
    "def r2_score(true, pred):\n",
    "    RSS = np.sum((true - pred)**2)\n",
    "    TSS = np.sum((true - np.mean(true))**2)\n",
    "    return 1 - RSS / TSS if TSS != 0 else s0\n",
    "\n",
    "print(\"Train R²: {:.2f} %\".format(r2_score(Iz_train_true.ravel(), pred_train_full.ravel()) * 100))\n",
    "print(\"Test R²: {:.2f} %\".format(r2_score(Iz_test_true.ravel(), pred_test_full.ravel()) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_pred = pred_test_full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(9, 3))\n",
    "im1 = ax1.imshow(pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], cmap = \"jet\",aspect='auto', vmin = 0, vmax = 600)\n",
    "# ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "ax1.set(ylabel=\"y [pix]\")\n",
    "ax1.set(xlabel = \"Time [fs]\")\n",
    "# ax1.set_title('True', fontsize = 12)\n",
    "# ax1.set(title = \"True\", fontsize = 2)\n",
    "ax1.set(xlim = (0,2*xrange))\n",
    "ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "im2 = ax2.imshow(PCA_pred.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], cmap = \"jet\",aspect='auto',vmin = 0, vmax = 600)\n",
    "ax2.set(xlabel = \"Time [fs]\")\n",
    "ax2.set(ylabel = \"y [pix]\")\n",
    "# ax2.set_title('Prediction', fontsize = 12)\n",
    "ax2.set(xlim = (0,2*xrange))\n",
    "ax2.set(ylim= (0,2*yrange))\n",
    "cbar = fig.colorbar(im1, ax=[ax1, ax2], fraction=0.16, pad=0.04)\n",
    "plt.suptitle(f'Test Set LPS Image: Shot {ntest[idx]}', fontsize = 12 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of PCA (before MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "fig, (ax2, ax3, cx2) = plt.subplots(1,3,figsize=(15, 3), gridspec_kw={'width_ratios': [1, 1, 0.02]})\n",
    "before_pca_image = Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx]\n",
    "#Flip in y direction for proper visualization\n",
    "im2 = ax2.imshow(np.flip(before_pca_image, axis=0), cmap = \"jet\",aspect='auto', vmin = 0, vmax = 400)\n",
    "# ax2.suptitle(f\"Current Profile Index: {idx}\")\n",
    "ax2.set(ylabel=\"y [pix]\")\n",
    "ax2.set(xlabel = \"Time [fs]\")\n",
    "ax2.set_title('Before PCA', fontsize = 12)\n",
    "# ax2.set(title = \"True\", fontsize = 2)\n",
    "ax2.set(xlim = (0,2*xrange))\n",
    "ax2.set(ylim= (0,2*yrange))\n",
    "\n",
    "after_pca_image = pca.inverse_transform(pca.transform(before_pca_image.flatten()[np.newaxis,:])).reshape(2*yrange,2*xrange)\n",
    "\n",
    "im3 = ax3.imshow(np.flip(after_pca_image, axis=0), cmap = \"jet\",aspect='auto', vmin = 0, vmax = 400)\n",
    "ax3.set(xlabel = \"Time [fs]\")\n",
    "ax3.set(ylabel = \"y [pix]\")\n",
    "ax3.set_title('After PCA', fontsize = 12)\n",
    "ax3.set(xlim = (0,2*xrange))\n",
    "ax3.set(ylim= (0,2*yrange))\n",
    "fig.colorbar((im2), cax=cx2, format='%.3g')\n",
    "fig.subplots_adjust(wspace=0.8)\n",
    "#fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "def plot_xtcav_image(idx):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(9, 3))\n",
    "    im1 = ax1.imshow(np.flip(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0), cmap = \"jet\",aspect='auto', vmin = 0, vmax = 600)\n",
    "   \n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    ax1.set(ylabel=\"y [pix]\")\n",
    "    ax1.set(xlabel = \"Time [fs]\")\n",
    "    ax1.set(title = \"True\")\n",
    "    ax1.set(xlim = (0,2*xrange))\n",
    "    ax1.set(ylim= (0,2*yrange))\n",
    "\n",
    "    im2 = ax2.imshow(np.flip(pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis=0), cmap = \"jet\",aspect='auto',vmin = 0, vmax = 600)\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "    ax2.set(xlim = (0,2*xrange))\n",
    "    ax2.set(ylim= (0,2*yrange))\n",
    "    cbar = fig.colorbar(im1, ax=[ax1, ax2], fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streakedProfile = horz_proj[:,idx]\n",
    "\n",
    "tvar = np.arange(1, len(streakedProfile) + 1) * xtcalibrationfactor\n",
    "tvar = tvar - np.median(tvar)  # Center around zero\n",
    "\n",
    "prefactor = charge[0, idx] / np.trapz(streakedProfile, tvar)\n",
    "\n",
    "currentProfile = 1e-3 * streakedProfile * prefactor  # Convert to kA\n",
    "currentProfile_all.append(currentProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "def plot_xtcav_image(idx):\n",
    "\n",
    "    horz_proj_true = np.sum(Iz_test_true.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis = 0)\n",
    "    tvar = np.arange(1, len(horz_proj_true) + 1) * xtcalibrationfactor\n",
    "    tvar = tvar - np.median(tvar) \n",
    "    prefactor = charge[0, ntest[idx]] / np.trapezoid(horz_proj_true, tvar) # needs to be idx from entire data set ==> manual train/test split with test_idx\n",
    "    currentProfile = 1e-3 * horz_proj_true * prefactor \n",
    "    plt.plot(currentProfile, label = \"True\", alpha = 0.5)\n",
    "    # ax1.suptitle(f\"Current Profile Index: {idx}\")\n",
    "    plt.ylabel(\"y [pix]\")\n",
    "    plt.xlabel(\"Time [fs]\")\n",
    "    # ax1.set(xlim = (0,200))\n",
    "    # ax1.set(ylim= (0,200))\n",
    "\n",
    "    horz_proj_pred = np.sum(pred_test_full.T.reshape(2*yrange,2*xrange,Iz_test_true.shape[0])[:,:,idx], axis = 0)\n",
    "    prefactor = charge[0, ntest[idx]] / np.trapezoid(horz_proj_pred, tvar)\n",
    "    currentProfile = 1e-3 * horz_proj_pred * prefactor \n",
    "    plt.plot(currentProfile, label = \"Prediction\", alpha = 0.5)\n",
    "    # ax2.set(xlim = (0,200))\n",
    "    # ax2.set(ylim= (0,200))\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    # plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtcav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
