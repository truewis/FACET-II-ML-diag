{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload when refreshing notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# import Python functions \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from Python_Functions.functions import matstruct_to_dict, extractDAQBSAScalars, apply_tcav_zeroing_filter\n",
    "from Python_Functions.gmm import biGaussian_image_from_flattened_params, flatten_biGaussian_params, unflatten_biGaussian_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat \n",
    "import re\n",
    "import os \n",
    "import joblib\n",
    "# Assumed: commonIndexFromSteps, extractDAQBSAScalars, and other helper functions are available\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0. Load the model with joblib\n",
    "# Ex: MLP_LPS_GMM_E338_12710_20251031_194740\n",
    "\n",
    "\n",
    "joblib_file = '../../model/LPS/MLP_LPS_GMM_Forest_E338_12710_20251103_082505.pkl'  # Modify as needed\n",
    "model = joblib.load(joblib_file)\n",
    "iz_scaler = pickle.load(open('../../model/LPS/E338_12710_iz_scaler_GMM_20251103_082505.pkl', 'rb'))\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Define the list of (experiment, runname, step_identifier) pairs to test the model on.\n",
    "# ----------------------------------------------------------------------\n",
    "run_pairs = [\n",
    "    ('E338', '12716', 1),  # Example pairs, modify this list\n",
    "    #('E300', '12431', 1),\n",
    "    # Add more pairs here...\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Initialize lists for concatenation\n",
    "# ----------------------------------------------------------------------\n",
    "all_predictors = []\n",
    "all_indices = []\n",
    "\n",
    "print(\"Starting multi-run data loading and concatenation...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Loop through runs, load data, and concatenate\n",
    "# ----------------------------------------------------------------------\n",
    "for experiment, runname, step_id in run_pairs:\n",
    "        \n",
    "    # --- B. Load and Filter Predictor Data (BSA Scalars) ---\n",
    "    \n",
    "    # 1. Load data_struct\n",
    "    dataloc = f'../../data/raw/{experiment}/{experiment}_{runname}/{experiment}_{runname}.mat'\n",
    "    try:\n",
    "        mat = loadmat(dataloc,struct_as_record=False, squeeze_me=True)\n",
    "        data_struct = mat['data_struct']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: .mat file not found at {dataloc}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Extract full BSA scalars (filtered by step_list if needed)\n",
    "    # Don't filter by common index here, we'll do it with the goodShots scalar common index loaded from the file\n",
    "    bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct, filter_index=False)\n",
    "    bsaScalarData = apply_tcav_zeroing_filter(bsaScalarData, bsaVars)\n",
    "\n",
    "    ampl_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_A' in var)\n",
    "    xtcavAmpl = bsaScalarData[ampl_idx, :]\n",
    "\n",
    "    phase_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_P' in var)\n",
    "    xtcavPhase = bsaScalarData[phase_idx, :]\n",
    "    xtcavOffShots = xtcavAmpl<0.1\n",
    "    xtcavPhase[xtcavOffShots] = 0 #Set this for ease of plotting\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(xtcavAmpl, label='Amplitude', color='b')\n",
    "    ax1.set_ylabel('XTCAV Ampl [MV]', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(xtcavPhase, label='Phase', color='r')\n",
    "    ax2.set_ylabel('XTCAV Phase [deg]', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    plt.title('XTCAV Amplitude and Phase')\n",
    "    plt.show()\n",
    "    # 5. Filter BSA data using the final index\n",
    "    # goodShots_scal_common_index is 1 based indexing from MATLAB, convert to 0 based\n",
    "    bsaScalarData_filtered = bsaScalarData\n",
    "    \n",
    "    # 6. Construct the predictor array\n",
    "    predictor_current = np.vstack(bsaScalarData_filtered).T\n",
    "    \n",
    "    # C. Append to master lists\n",
    "    all_predictors.append(predictor_current)\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Concatenate and finalize arrays\n",
    "# ----------------------------------------------------------------------\n",
    "# Combine all data arrays from the runs\n",
    "predictor_tmp = np.concatenate(all_predictors, axis=0)\n",
    "\n",
    "# Set image half dimensions (should match preprocessing)\n",
    "yrange = 50\n",
    "xrange = 150\n",
    "NCOMP = 10  # Number of GMM parameters\n",
    "print(\"\\n--- Final Concatenated Data Shapes ---\")\n",
    "print(f\"Total Predictors (predictor): {predictor_tmp.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude BSA Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Python_Functions.functions import exclude_bsa_vars\n",
    "excluded_var_idx = exclude_bsa_vars(bsaVars)\n",
    "predictor_tmp_cleaned = np.delete(predictor_tmp, excluded_var_idx, axis=1)\n",
    "bsaVars_cleaned = [var for i, var in enumerate(bsaVars) if i not in excluded_var_idx]\n",
    "print(f\"Predictor shape after excluding variables: {predictor_tmp_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XTCAV calibration\n",
    "krf = 239.26\n",
    "cal = 1167 # um/deg  http://physics-elog.slac.stanford.edu/facetelog/show.jsp?dir=/2025/11/13.03&pos=2025-$\n",
    "streakFromGUI = cal*krf*180/np.pi*1e-6#um/um\n",
    "xtcalibrationfactor = 6.35e-15\n",
    "\n",
    "isChargePV = [bool(re.search(r'TORO_LI20_2452_TMIT', pv)) for pv in bsaVars]\n",
    "if isChargePV:\n",
    "    # Extract charge data\n",
    "    pvidx = [i for i, val in enumerate(isChargePV) if val]\n",
    "    charge = bsaScalarData[pvidx, :][0] * 1.6e-19  # in C \n",
    "    charge_filtered = charge\n",
    "\n",
    "# Set flag for current profile fitting. If True, the current profile will be used to refine the GMM fit.\n",
    "do_current_profile = True\n",
    "NCOMP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# --- Original scaling and splitting logic follows ---\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_scaled = x_scaler.fit_transform(predictor_tmp_cleaned)\n",
    "\n",
    "# all dataset is used for testing purpose here\n",
    "x_test_scaled = x_scaled\n",
    "ntest = np.arange(x_test_scaled.shape[0])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "# Evaluate model\n",
    "pred_test_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "pred_test_full = iz_scaler.inverse_transform(pred_test_scaled)\n",
    "#pca.inverse_transform(pred_test_scaled)\n",
    "# Compute R² score\n",
    "def r2_score(true, pred):\n",
    "    RSS = np.sum((true - pred)**2)\n",
    "    TSS = np.sum((true - np.mean(true))**2)\n",
    "    return 1 - RSS / TSS if TSS != 0 else s0\n",
    "\n",
    "print(\"Test R² is Unknown (Iz_test_true not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred(idx):\n",
    "    fig, (ax2, cx1) = plt.subplots(1,2,figsize=(8, 6), gridspec_kw={'width_ratios': [1, 0.02]})\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    pred_im = biGaussian_image_from_flattened_params(pred_params, total_charge=charge_filtered[idx] if do_current_profile else None, yrange=yrange, xrange=xrange)\n",
    "    pred_im = pred_im.T\n",
    "    im2 = ax2.imshow(pred_im*1e15, cmap = \"jet\", extent=(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, 0, 2*yrange), aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = f\"LPS Prediction: {experiment}_{runname}, Shot Number: {ntest[idx]}\")\n",
    "    cbar = fig.colorbar(im2, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    cbar.set_label(\"Current [A per y pixel]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    print(f\"Displaying prediction for index: {idx}\")\n",
    "    print(f\"Parameters: {unflatten_biGaussian_params(pred_params)}\")\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred_current(idx):\n",
    "    FACTOR = 1e15\n",
    "    fig, (ax0) = plt.subplots(1,1,figsize=(10, 6))\n",
    "\n",
    "    x_grid = np.linspace(-xrange*xtcalibrationfactor*FACTOR, xrange*xtcalibrationfactor*FACTOR, xrange * 2)\n",
    "\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    # Current profile is relative to zeta; transpose so that the drive bunch center agrees with the fit.\n",
    "    pred_params_uf = unflatten_biGaussian_params(pred_params)\n",
    "    pred_params = flatten_biGaussian_params(pred_params_uf)\n",
    "    \n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[ntest[idx]]\n",
    "        pred_im = biGaussian_image_from_flattened_params(pred_params, charge_value, xrange=xrange, yrange=yrange)\n",
    "    else:\n",
    "        pred_im = biGaussian_image_from_flattened_params(pred_params, xrange=xrange, yrange=yrange)\n",
    "    pred_im = pred_im.T\n",
    "    pred_im_proj = np.sum(pred_im, axis = 0) * FACTOR\n",
    "    ax0.plot(x_grid, pred_im_proj, label = \"prediction\")\n",
    "    # Fix x-axis limits\n",
    "    ax0.set(xlim = (-xrange * xtcalibrationfactor * FACTOR, xrange * xtcalibrationfactor * FACTOR))\n",
    "    ax0.set(ylim=(1e2, None))\n",
    "    ax0.legend()\n",
    "\n",
    "    print(\"Predicted Gaussian Parameters:\")\n",
    "    pred_ug = unflatten_biGaussian_params(pred_params)\n",
    "    print(\"Means (Mu):\", pred_ug['mu'].numpy())\n",
    "    print(\"Covariances (Sigma):\", pred_ug['Sigma'].numpy())\n",
    "    print(\"Weights (Pi):\", pred_ug['pi'].numpy())\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "\n",
    "    # Display image_to_bigaussian_params debug info\n",
    "    #biGaussianTest = image_to_bigaussian_params(images[valid_rows][ntest[idx]].reshape((2*yrange, 2*xrange)), do_current_profile, debug=True)\n",
    "    #print(\"Bi-Gaussian Parameters for Test Image:\", biGaussianTest)\n",
    "    \n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred_current, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='80%')));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtcav_image_mu(idx):\n",
    "    FACTOR = 1e15\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "\n",
    "    pred_ug = unflatten_biGaussian_params(pred_params)\n",
    "    return (pred_ug['mu'][0][0]-pred_ug['mu'][1][0]) * xtcalibrationfactor * FACTOR  # The difference in horizontal means (mu_A_x - mu_B_x)\n",
    "mu_list = []\n",
    "for i in range(pred_test_full.shape[0]):\n",
    "    mu_list.append(xtcav_image_mu(i))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(mu_list)\n",
    "plt.xlabel('Shot Index')\n",
    "plt.ylabel('Distance between Centroids (fs)')\n",
    "plt.title('Predicted Time between Two Gaussian Components in {}'.format(f\"{experiment}_{runname}\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of BC14 BLEN vs predicted peak separation\n",
    "pvidx = bsaVars.index('BLEN_LI14_888_BRAW')\n",
    "bc14_blen = predictor_tmp[:, pvidx]\n",
    "peak_separations = mu_list\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(bc14_blen, peak_separations, alpha=0.5)\n",
    "r_squared = np.corrcoef(bc14_blen, peak_separations)[0, 1]**2\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "plt.xlabel('BC14 BLEN')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title(f'Predicted Peak Separation vs BC14 BLEN: {experiment}_{runname}')\n",
    "plt.xlim(5000, 12500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Python_Functions.functions import analyze_SYAG\n",
    "syag_data = analyze_SYAG(data_struct, experiment=experiment, runname=runname, skipEOSanalysis=False, SYAGxmin=800, SYAGxmax=950, mindels=50, maxdels=500, min_prom = 1000, goosing=False, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYAG del vs shot index\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(syag_data['dels'])\n",
    "plt.xlabel('Shot Index')\n",
    "plt.ylabel('SYAG del [pixels]')\n",
    "plt.title(f'SYAG del vs Shot Index: {experiment}_{runname}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of SYAG del vs predicted peak separation\n",
    "plt.figure(figsize=(8,6))\n",
    "print(syag_data['dels'].size)\n",
    "print(len(peak_separations))\n",
    "plt.scatter(syag_data['dels'], peak_separations, alpha=0.5)\n",
    "r_squared_syag = np.corrcoef(syag_data['dels'], peak_separations)[0, 1]**2\n",
    "print(f\"R-squared (SYAG del vs predicted peak separation): {r_squared_syag}\")\n",
    "plt.xlabel('SYAG Measured Peak Separation (pix)')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title(f'Predicted Peak Separation vs SYAG Measured Peak Separation: {experiment}_{runname}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of SYAG del vs predicted peak separation, filtered for blen value range\n",
    "blen_filter_min = 9500\n",
    "blen_filter_max = 9600\n",
    "# Filter based on blen values\n",
    "blen_filter = (bc14_blen >= blen_filter_min) & (bc14_blen <= blen_filter_max)\n",
    "plt.figure(figsize=(8,6))\n",
    "print(syag_data['dels'].size)\n",
    "print(len(peak_separations))\n",
    "plt.scatter(np.array(syag_data['dels'])[blen_filter], np.array(peak_separations)[blen_filter], alpha=0.5)\n",
    "r_squared_syag = np.corrcoef(np.array(syag_data['dels'][blen_filter]), np.array(peak_separations)[blen_filter])[0, 1]**2\n",
    "print(f\"R-squared (SYAG del vs predicted peak separation): {r_squared_syag}\")\n",
    "plt.xlabel('SYAG Measured Peak Separation (pix)')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title(f'Predicted Peak Separation vs SYAG Measured Peak Separation: {experiment}_{runname}')\n",
    "plt.xlim(50, 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of BC14 BLEN vs predicted peak separation\n",
    "step_id = 3\n",
    "shots = [i for i in range(250 * step_id + 10, 250 * (step_id + 1) -10)]\n",
    "bc14_blen = np.array(bc14_blen)\n",
    "peak_separations = np.array(peak_separations)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(bc14_blen[shots], peak_separations[shots], alpha=0.5)\n",
    "r_squared = np.corrcoef(bc14_blen[shots], peak_separations[shots])[0, 1]**2\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "plt.xlabel('BC14 BLEN')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title(f'Predicted Peak Separation vs BC14 BLEN: {experiment}_{runname}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BLEN BC14 vs shot index\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(bc14_blen)\n",
    "plt.xlabel('Shot Index')\n",
    "plt.ylabel('BC14 BLEN')\n",
    "plt.title(f'BC14 BLEN vs Shot Index: {experiment}_{runname}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of PV Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bsaScalarData TCAV_LI20_2400_P and TCAV_LI20_2400_A, most important predictors.\n",
    "bsaVarNames = bsaVars\n",
    "var1_name = 'TCAV_LI20_2400_P'\n",
    "var2_name = 'TCAV_LI20_2400_A'\n",
    "var1_idx = bsaVarNames.index(var1_name)\n",
    "var2_idx = bsaVarNames.index(var2_name)\n",
    "print(f\"Plotting BSA Scalars: {var1_name} (index {var1_idx}) and {var2_name} (index {var2_idx})\")\n",
    "plt.figure(figsize=(8,6))\n",
    "# Very small dot and alpha\n",
    "plt.scatter(predictor_tmp[:, var1_idx], predictor_tmp[:, var2_idx], alpha=0.1, s=1)\n",
    "plt.xlabel(var1_name)\n",
    "plt.ylabel(var2_name)\n",
    "plt.title('BSA Scalar Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming bsaScalarData has shape (N_variables, N_samples) from your function\n",
    "# Transpose the data so features are columns and samples are rows for scikit-learn PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(predictor_tmp_cleaned)\n",
    "\n",
    "\n",
    "# Determine the maximum number of components to test\n",
    "# X.shape is (N_samples, N_variables). max_components = min(N_samples, N_variables) - 1 for a stable PCA\n",
    "max_components = min(X.shape) // 5 \n",
    "\n",
    "# Lists to store results\n",
    "n_components_list = []\n",
    "reconstruction_losses = []\n",
    "\n",
    "# Loop through possible number of components\n",
    "for k in range(1, max_components + 1):\n",
    "    # 1. Initialize and fit PCA\n",
    "    pca_study = PCA(n_components=k)\n",
    "    pca_study.fit(X)\n",
    "    \n",
    "    # 2. Transform and Inverse Transform (Reconstruct)\n",
    "    X_reduced = pca_study.transform(X)\n",
    "    X_reconstructed = pca_study.inverse_transform(X_reduced)\n",
    "    \n",
    "    # 3. Calculate Reconstruction Loss (Mean Squared Error)\n",
    "    loss = mean_squared_error(X, X_reconstructed)\n",
    "    \n",
    "    # Store results\n",
    "    n_components_list.append(k)\n",
    "    reconstruction_losses.append(loss)\n",
    "    \n",
    "# 4. Plot the Results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(n_components_list, reconstruction_losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Reconstruction Loss (Mean Squared Error)')\n",
    "# Log scale in y-axis for better visualization\n",
    "plt.yscale('log')\n",
    "plt.title('PCA of BSA Scalars: Reconstruction Loss vs. Number of Components')\n",
    "plt.grid(True)\n",
    "# 5. Identify the \"Elbow\" point visually after plotting plt.show()\n",
    "plt.show() \n",
    "\n",
    "# After plotting, the optimal number of components is the 'elbow' point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_components = 8\n",
    "\n",
    "pca_comp_study = PCA(n_components=num_components)\n",
    "pca_comp_study.fit(X)\n",
    "# 1. Get explained variance and component loadings\n",
    "\n",
    "variance_ratios = pca_comp_study.explained_variance_ratio_\n",
    "loadings = pca_comp_study.components_\n",
    "\n",
    "# 2. Create the DataFrame for component composition\n",
    "# Each row in 'loadings' is a principal component (PC)\n",
    "# Each column corresponds to a feature (PV)\n",
    "df_loadings = pd.DataFrame(loadings, columns=bsaVars_cleaned)\n",
    "\n",
    "# 3. Add Component labels and Significance\n",
    "component_labels = [f'PC {i+1}' for i in range(num_components)]\n",
    "df_loadings.insert(0, 'Component', component_labels)\n",
    "df_loadings.insert(1, 'Significance (Explained Variance Ratio)', variance_ratios)\n",
    "\n",
    "# 4. Format the output\n",
    "# The components are already ordered by significance (PC 1 is most significant)\n",
    "# Format the significance column as a percentage for clarity\n",
    "df_loadings['Significance (Explained Variance Ratio)'] = \\\n",
    "    df_loadings['Significance (Explained Variance Ratio)'].map(lambda x: f'{x:.4f} ({x*100:.2f}%)')\n",
    "\n",
    "# Format the loadings to a fixed number of decimal places\n",
    "for col in bsaVars_cleaned:\n",
    "    df_loadings[col] = df_loadings[col].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "composition_table = df_loadings.to_markdown(index=False)\n",
    "print(\"Composition of Principal Components:\")\n",
    "print(composition_table)\n",
    "# In the composition table, leave the most significant value in each column.\n",
    "composition_table_significant = df_loadings.copy()\n",
    "for col in bsaVars_cleaned:\n",
    "    # Find index of two max absolute loadings for this column\n",
    "    max_idx_two = df_loadings[col].astype(float).abs().sort_values(ascending=False).index[0:2].to_list()\n",
    "    # Set all other values to empty string, leaving only the top two\n",
    "    for idx in range(len(composition_table_significant)):\n",
    "        if idx not in max_idx_two:\n",
    "            composition_table_significant.at[idx, col] = ''\n",
    "\n",
    "composition_table_significant = composition_table_significant.to_markdown(index=False)\n",
    "print(\"Most Significant Loadings in Each Column:\")\n",
    "print(composition_table_significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# 1. Prepare the data for plotting\n",
    "# The dataframe df_loadings_raw should have Components as the index and Variables as columns\n",
    "\n",
    "df_loadings_raw = df_loadings.copy()\n",
    "plot_data = df_loadings_raw\n",
    "plot_data = plot_data.transpose() # Transpose to get Variables on Y-axis (often clearer)\n",
    "# Debug prints\n",
    "# print(plot_data.head())\n",
    "# print(plot_data.dtypes)\n",
    "# Remove \"Component\" and \"Significance\" rows for heatmap\n",
    "plot_data = plot_data.drop(index=['Component', 'Significance (Explained Variance Ratio)'])\n",
    "# Order the rows by original variable names z positions\n",
    "z_positions = []\n",
    "for var in plot_data.index:\n",
    "    match = re.search(r'BPM_LI(\\d+)_\\d+_(\\d+)', var)\n",
    "    if match:\n",
    "        z_pos = int(match.group(1))\n",
    "    else:\n",
    "        z_pos = float('inf')  # Place non-matching variables at the end\n",
    "    z_positions.append((var, z_pos))\n",
    "# Sort by z position\n",
    "z_positions.sort(key=lambda x: x[1])\n",
    "sorted_vars = [var for var, _ in z_positions]\n",
    "plot_data = plot_data.reindex(sorted_vars)\n",
    "# Convert all data to float for heatmap\n",
    "plot_data = plot_data.astype(float)\n",
    "# 2. Create the heatmap\n",
    "plt.figure(figsize=(12, 24))\n",
    "sns.heatmap(\n",
    "    plot_data,\n",
    "    cmap='vlag',            # 'vlag' or 'coolwarm' are good diverging colormaps (blue/red)\n",
    "    center=0,               # Ensures white/transparent is at 0\n",
    "    linewidths=0.5,\n",
    "    linecolor='lightgray',\n",
    "    annot=False,            # Set to True to display loading values in cells\n",
    "    cbar_kws={'label': 'Loading Value (Correlation)'}\n",
    ")\n",
    "\n",
    "plt.title('Heatmap of Principal Component Loadings', fontsize=16)\n",
    "plt.xlabel('Principal Component', fontsize=12)\n",
    "plt.ylabel('Original Variable', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right') # Rotate X-labels for better readability\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout() # Adjust layout to prevent labels from being cut off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract and Square the Loadings (Loadings matrix is: Components x Variables)\n",
    "# loadings = pca_comp_study.components_ (from your provided code block)\n",
    "squared_loadings = loadings**2\n",
    "\n",
    "# 2. Sum Across Components (Sum columns to get total significance per variable)\n",
    "# The result is an array where each element is the total squared loading for a variable\n",
    "total_squared_loadings = np.sum(squared_loadings, axis=0)\n",
    "\n",
    "# 3. Create a DataFrame for sorting\n",
    "# bsaVars is the list of variable names (features)\n",
    "df_var_significance = pd.DataFrame({\n",
    "    'BSA Variable (PV)': bsaVars_cleaned,\n",
    "    'Total Squared Loading (Significance)': total_squared_loadings\n",
    "})\n",
    "\n",
    "# 4. Sort in descending order\n",
    "df_var_significance = df_var_significance.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 5. Specify the number of top variables to display\n",
    "N_top = 20  # Example: display the top few most significant variables\n",
    "\n",
    "# 6. Format and display the table\n",
    "df_top_vars = df_var_significance.head(N_top)\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "print(f\"Top {N_top} BSA Variables Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_var_significance[\n",
    "    ~df_var_significance['BSA Variable (PV)'].str.contains('BPM', case=False, na=False)\n",
    "]\n",
    "\n",
    "# 5. Sort the filtered variables in descending order of significance\n",
    "df_filtered = df_filtered.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 6. Select Top N and format\n",
    "N_top = 20 # Display the top few most significant non-BPM variables\n",
    "df_top_vars = df_filtered.head(N_top)\n",
    "\n",
    "# Format the significance column\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "\n",
    "print(f\"Top {N_top} BSA Variables (EXCLUDING BPMS) Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance to nearest real sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatSlider, Layout, interact\n",
    "index_to_examine = 82  # Change this index to examine different test samples\n",
    "pca_vals = pca_comp_study.transform(X_test.numpy())\n",
    "# Variables to be controlled (Non-BPM variables from previous ranking logic)\n",
    "# ==============================================================================\n",
    "# --- DYNAMIC PREDICTION FUNCTION ---\n",
    "# ==============================================================================\n",
    "original_predictor_reconstructed = x_scaler.inverse_transform(X_test.numpy())\n",
    "def plot_dynamic_pred(**slider_values):\n",
    "    idx = index_to_examine\n",
    "    fig, (ax2,ax3, cx1) = plt.subplots(1,3,figsize=(12, 5), gridspec_kw={'width_ratios': [1,1, 0.02]})\n",
    "    \n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    pred_im = biGaussian_image_from_flattened_params(pred_params).T\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[ntest[idx]]\n",
    "        pred_im_sum = np.sum(pred_im)\n",
    "        pred_im = pred_im / pred_im_sum * charge_value\n",
    "    im2 = ax2.imshow(pred_im*1e15, cmap = \"jet\", extent=(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, 0, 2*yrange), aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = \"Prediction\")\n",
    "\n",
    "    # Modify the predictor for the given index based on slider values\n",
    "    pca_components = []\n",
    "    for var_name, slider in slider_values.items():\n",
    "        # Reconstruct the original predictor values from PCA components\n",
    "        pca_components.append(slider)\n",
    "    pca_components = np.array(pca_components).reshape(1, -1)\n",
    "    modified_predictor = pca_comp_study.inverse_transform(pca_components).flatten()\n",
    "\n",
    "\n",
    "    # Scale back the modified predictor\n",
    "    modified_predictor_scaled = modified_predictor.reshape(1, -1)\n",
    "\n",
    "    # Make prediction with modified predictor\n",
    "    modified_predictor_tensor = torch.tensor(modified_predictor_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        modified_pred_scaled = model.predict(modified_predictor_tensor)\n",
    "    modified_pred_full = iz_scaler.inverse_transform(modified_pred_scaled)\n",
    "    modified_pred_params = modified_pred_full[0]\n",
    "\n",
    "    # Print the modified parameters minus the original predicted parameters for comparison\n",
    "    print(\"Difference in Parameters from Original Prediction:\")\n",
    "    for i, name in enumerate([\"pi_1\", \"pi_2\", \"mu_1_x\", \"mu_1_y\", \"sigma_1_xx\", \"sigma_1_xy\", \"sigma_1_yy\", \"sigma_2_xx\", \"sigma_2_xy\", \"sigma_2_yy\"]):\n",
    "        diff = modified_pred_params[i] - pred_params[i]\n",
    "        print(f\"{name}: {diff:.4f}\")\n",
    "\n",
    "    # Print the euclidean distance to the nearest training point\n",
    "    nearest_point = pca_vals[np.argmin(np.linalg.norm(pca_vals - pca_components, axis=1))]\n",
    "    nearest_dist = np.linalg.norm(nearest_point - pca_components)\n",
    "    print(f\"Euclidean Distance to Nearest Training Point: {nearest_dist:.4f}\")\n",
    "    # Nearest point\n",
    "    print(f\"Nearest Training Point: {nearest_point}\")\n",
    "    modified_pred_im = biGaussian_image_from_flattened_params(modified_pred_params).T\n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[ntest[idx]]\n",
    "        modified_pred_im_sum = np.sum(modified_pred_im)\n",
    "        modified_pred_im = modified_pred_im / modified_pred_im_sum * charge_value\n",
    "    im3 = ax3.imshow(modified_pred_im, cmap = \"jet\",extent=(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, 0, 2*yrange), aspect='auto')\n",
    "    ax3.set(xlabel = \"Time [fs]\")\n",
    "    ax3.set(ylabel = \"y [pix]\")\n",
    "    ax3.set(title = \"Modified Prediction\")\n",
    "\n",
    "    cbar = fig.colorbar(im2, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "\n",
    "N_SAMPLES = pred_test_full.shape[0]\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "for i in range(pca_comp_study.components_.shape[0]):\n",
    "    pv_name = f\"PC{i+1}\"\n",
    "    pv_index = i\n",
    "    min_val, max_val = pca_vals[:, pv_index].min(), pca_vals[:, pv_index].max()\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    initial_value = pca_vals[index_to_examine, pv_index] # Use the value for the initial index (idx=0)\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value,\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_dynamic_pred, **widget_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatSlider, Layout, interact\n",
    "index_to_examine = 82  # Change this index to examine different test samples\n",
    "pca_vals = pca_comp_study.transform(X_test.numpy())\n",
    "# Variables to be controlled (Non-BPM variables from previous ranking logic)\n",
    "# ==============================================================================\n",
    "# --- DYNAMIC PREDICTION FUNCTION ---\n",
    "# ==============================================================================\n",
    "original_predictor_reconstructed = x_scaler.inverse_transform(X_test.numpy())\n",
    "def plot_dynamic_pred(**slider_values):\n",
    "    idx = index_to_examine\n",
    "    # Modify the predictor for the given index based on slider values\n",
    "    pca_components = []\n",
    "    for var_name, slider in slider_values.items():\n",
    "        # Reconstruct the original predictor values from PCA components\n",
    "        pca_components.append(slider)\n",
    "    pca_components = np.array(pca_components).reshape(1, -1)\n",
    "    modified_predictor = pca_comp_study.inverse_transform(pca_components).flatten()\n",
    "    # Plot BPMS X values, ordered by z position (4 digit integer in PV name)\n",
    "    fig, (ax2,ax3) = plt.subplots(2,1,figsize=(12, 5))\n",
    "    # Find BPMS names within bsaVars_cleaned\n",
    "    bpm_x_indices = [i for i, var in enumerate(bsaVars_cleaned) if re.search(r'BPMS.*_X$', var)]\n",
    "    # Extract z positions and sort\n",
    "    bpm_z_positions = [int(re.search(r'BPMS.*_(\\d+)_X$', bsaVars_cleaned[i]).group(1)) for i in bpm_x_indices]\n",
    "    # Print bpm_z_positions for debugging\n",
    "    print(\"BPM Z Positions:\", bpm_z_positions)\n",
    "    # print values for debugging\n",
    "    print(\"Modified Predictor BPM X Values:\", modified_predictor[bpm_x_indices])\n",
    "    sorted_bpm_indices = [x for _, x in sorted(zip(bpm_z_positions, bpm_x_indices))]\n",
    "    # Plot sorted_bpm_indices, at their z positions\n",
    "    # These are nomalized between 0 and 1, shift down by 0.5 for visibility\n",
    "    ax2.bar(bpm_z_positions, modified_predictor[sorted_bpm_indices]-0.5, color='green', label='BPM X Corresponding Values')\n",
    "    ax2.set_title(\"Modified Predictor Values (BPM X highlighted)\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "    ax2.set(ylim = (-0.5, 0.5))\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot Y values\n",
    "    bpm_y_indices = [i for i, var in enumerate(bsaVars_cleaned) if re.search(r'BPMS.*_Y$', var)]\n",
    "    # Extract z positions and sort\n",
    "    bpm_z_positions_y = [int(re.search(r'BPMS.*_(\\d+)_Y$', bsaVars_cleaned[i]).group(1)) for i in bpm_y_indices]\n",
    "    sorted_bpm_indices_y = [x for _, x in sorted(zip(bpm_z_positions_y, bpm_y_indices))]\n",
    "    ax3.bar(bpm_z_positions, modified_predictor[sorted_bpm_indices_y]-0.5, color='green', label='BPM Y Corresponding Values')\n",
    "    ax3.set_title(\"Modified Predictor Values (BPM Y highlighted)\")\n",
    "    ax3.set_ylabel(\"Value\")\n",
    "    ax3.set(ylim = (-0.5, 0.5))\n",
    "    ax3.legend()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INTERACTIVE WIDGET SETUP ---\n",
    "# ==============================================================================\n",
    "# 1. Calculate min/max for sliders (using the mock raw data)\n",
    "\n",
    "N_SAMPLES = pred_test_full.shape[0]\n",
    "# 2. Setup the Widgets\n",
    "widget_kwargs = {}  # Fixed index for demonstration\n",
    "\n",
    "for i in range(pca_comp_study.components_.shape[0]):\n",
    "    pv_name = f\"PC{i+1}\"\n",
    "    pv_index = i\n",
    "    min_val, max_val = pca_vals[:, pv_index].min(), pca_vals[:, pv_index].max()\n",
    "    # Initial value is the mean of the range, as we cannot predict \n",
    "    # the X_test[idx] value without dynamic input in the slider setup.\n",
    "    # We set it to 0.0 (or mid-range) and rely on the plot_dynamic_pred \n",
    "    # function to construct the input correctly.\n",
    "    initial_value = pca_vals[index_to_examine, pv_index] # Use the value for the initial index (idx=0)\n",
    "    \n",
    "    widget_kwargs[pv_name] = FloatSlider(\n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=(max_val - min_val) / 100.0, # 100 steps granularity\n",
    "        value=initial_value,\n",
    "        description=pv_name,\n",
    "        readout_format='.4f',\n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "\n",
    "print(\"--- Dynamic ML Prediction Explorer ---\")\n",
    "print(\"Adjust the sliders below to see the real-time effect on the LPS profile prediction.\")\n",
    "\n",
    "# 3. Create the Interactive Interface\n",
    "# Note: The widget values are passed as keyword arguments to plot_dynamic_pred\n",
    "interact(plot_dynamic_pred, **widget_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtcav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
