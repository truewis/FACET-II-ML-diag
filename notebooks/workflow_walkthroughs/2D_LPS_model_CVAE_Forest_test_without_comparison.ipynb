{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload when refreshing notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import scipy\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# import Python functions \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from Python_Functions.functions import matstruct_to_dict, extractDAQBSAScalars, apply_tcav_zeroing_filter, analyze_eos_and_cher, analyze_SYAG\n",
    "from Python_Functions.gmm import bigaussian_1d\n",
    "from Python_Functions.cvae import CVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat \n",
    "import re\n",
    "import os \n",
    "import joblib\n",
    "# Assumed: commonIndexFromSteps, extractDAQBSAScalars, and other helper functions are available\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0. Load the model with joblib\n",
    "# Ex: MLP_LPS_GMM_E338_12710_20251031_194740\n",
    "\n",
    "\n",
    "joblib_file = '../../model/LPS/MLP_LPS_CVAE_Forest_E300_12427_20251104_075246.pkl'  # Modify as needed\n",
    "model = joblib.load(joblib_file)\n",
    "joblib_file_cvae = '../../model/LPS/MLP_LPS_CVAE_E300_12427_20251104_075246.pkl'  # Modify as needed\n",
    "model_cvae = joblib.load(joblib_file_cvae)\n",
    "iz_scaler = pickle.load(open('../../model/LPS/E300_12427_iz_scaler_CVAE_20251104_075246.pkl', 'rb'))\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Define the list of (experiment, runname, step_identifier) pairs to test the model on.\n",
    "# ----------------------------------------------------------------------\n",
    "run_pairs = [\n",
    "    ('E300', '12405', 1),  # Example pairs, modify this list\n",
    "    #('E300', '12431', 1),\n",
    "    #('E300', '12405', 1),\n",
    "    # Add more pairs here...\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Initialize lists for concatenation\n",
    "# ----------------------------------------------------------------------\n",
    "all_predictors = []\n",
    "all_indices = []\n",
    "\n",
    "print(\"Starting multi-run data loading and concatenation...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Loop through runs, load data, and concatenate\n",
    "# ----------------------------------------------------------------------\n",
    "for experiment, runname, step_id in run_pairs:\n",
    "        \n",
    "    # --- B. Load and Filter Predictor Data (BSA Scalars) ---\n",
    "    \n",
    "    # 1. Load data_struct\n",
    "    dataloc = f'../../data/raw/{experiment}/{experiment}_{runname}/{experiment}_{runname}.mat'\n",
    "    try:\n",
    "        mat = loadmat(dataloc,struct_as_record=False, squeeze_me=True)\n",
    "        data_struct = mat['data_struct']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {experiment}_{runname}: .mat file not found at {dataloc}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Extract full BSA scalars (filtered by step_list if needed)\n",
    "    # Don't filter by common index here, we'll do it with the goodShots scalar common index loaded from the file\n",
    "    bsaScalarData, bsaVars = extractDAQBSAScalars(data_struct, filter_index=False)\n",
    "    bsaScalarData = apply_tcav_zeroing_filter(bsaScalarData, bsaVars)\n",
    "\n",
    "    ampl_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_A' in var)\n",
    "    xtcavAmpl = bsaScalarData[ampl_idx, :]\n",
    "\n",
    "    phase_idx = next(i for i, var in enumerate(bsaVars) if 'TCAV_LI20_2400_P' in var)\n",
    "    xtcavPhase = bsaScalarData[phase_idx, :]\n",
    "    xtcavOffShots = xtcavAmpl<0.1\n",
    "    xtcavPhase[xtcavOffShots] = 0 #Set this for ease of plotting\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(xtcavAmpl, label='Amplitude', color='b')\n",
    "    ax1.set_ylabel('XTCAV Ampl [MV]', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(xtcavPhase, label='Phase', color='r')\n",
    "    ax2.set_ylabel('XTCAV Phase [deg]', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    plt.title('XTCAV Amplitude and Phase')\n",
    "    plt.show()\n",
    "    # 3. \n",
    "\n",
    "    # 5. Filter BSA data using the final index\n",
    "    # goodShots_scal_common_index is 1 based indexing from MATLAB, convert to 0 based\n",
    "    bsaScalarData_filtered = bsaScalarData\n",
    "    \n",
    "    # 6. Construct the predictor array\n",
    "    predictor_current = np.vstack(bsaScalarData_filtered).T\n",
    "    \n",
    "    # C. Append to master lists\n",
    "    all_predictors.append(predictor_current)\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Concatenate and finalize arrays\n",
    "# ----------------------------------------------------------------------\n",
    "# Combine all data arrays from the runs\n",
    "predictor_tmp = np.concatenate(all_predictors, axis=0)\n",
    "\n",
    "# Set image half dimensions (should match preprocessing)\n",
    "yrange = 100\n",
    "xrange = 100\n",
    "NCOMP = 8  # Number of GMM parameters\n",
    "print(\"\\n--- Final Concatenated Data Shapes ---\")\n",
    "print(f\"Total Predictors (predictor): {predictor_tmp.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude BSA Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Python_Functions.functions import exclude_bsa_vars\n",
    "excluded_var_idx = exclude_bsa_vars(bsaVars)\n",
    "predictor_tmp_cleaned = np.delete(predictor_tmp, excluded_var_idx, axis=1)\n",
    "print(f\"Predictor shape after excluding variables: {predictor_tmp_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XTCAV calibration\n",
    "krf = 239.26\n",
    "cal = 1167 # um/deg  http://physics-elog.slac.stanford.edu/facetelog/show.jsp?dir=/2025/11/13.03&pos=2025-$\n",
    "streakFromGUI = cal*krf*180/np.pi*1e-6#um/um\n",
    "xtcalibrationfactor = 6.35e-15\n",
    "\n",
    "isChargePV = [bool(re.search(r'TORO_LI20_2452_TMIT', pv)) for pv in bsaVars]\n",
    "if isChargePV:\n",
    "    # Extract charge data\n",
    "    pvidx = [i for i, val in enumerate(isChargePV) if val]\n",
    "    charge = bsaScalarData[pvidx, :][0] * 1.6e-19  # in C \n",
    "    charge_filtered = charge\n",
    "\n",
    "# Set flag for current profile fitting. If True, the current profile will be used to refine the GMM fit.\n",
    "do_current_profile = True\n",
    "NCOMP = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(xtcavAmpl, label='Amplitude', color='b')\n",
    "ax1.set_ylabel('XTCAV Ampl [MV]', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xtcavPhase, label='Phase', color='r')\n",
    "ax2.set_ylabel('XTCAV Phase [deg]', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.title('XTCAV Amplitude and Phase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# --- Original scaling and splitting logic follows ---\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_scaled = x_scaler.fit_transform(predictor_tmp_cleaned)\n",
    "\n",
    "# all dataset is used for testing purpose here\n",
    "x_test_scaled = x_scaled\n",
    "ntest = np.arange(x_test_scaled.shape[0])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "# Evaluate model\n",
    "pred_test_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "pred_test_full = iz_scaler.inverse_transform(pred_test_scaled)\n",
    "#pca.inverse_transform(pred_test_scaled)\n",
    "# Compute R² score\n",
    "def r2_score(true, pred):\n",
    "    RSS = np.sum((true - pred)**2)\n",
    "    TSS = np.sum((true - np.mean(true))**2)\n",
    "    return 1 - RSS / TSS if TSS != 0 else s0\n",
    "\n",
    "print(\"Test R² is Unknown (Iz_test_true not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred(idx):\n",
    "    fig, (ax2, cx1) = plt.subplots(1,2,figsize=(8, 6), gridspec_kw={'width_ratios': [1, 0.02]})\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params, dtype=torch.float32).to(device)).cpu().detach().numpy().reshape(2*xrange, 2*yrange)\n",
    "    im2 = ax2.imshow(pred_im*1e15, cmap = \"jet\", extent=(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, 0, 2*yrange), aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = f\"LPS Prediction: {experiment}_{runname}, Shot Number: {ntest[idx]}\")\n",
    "    cbar = fig.colorbar(im2, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    cbar.set_label(\"Current [A per y pixel]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    print(f\"Displaying prediction for index: {idx}\")\n",
    "    print(f\"Parameters: {pred_params}\")\n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, Layout\n",
    "def plot_xtcav_image_pred_current(idx):\n",
    "    FACTOR = 1e15\n",
    "    fig, (ax0) = plt.subplots(1,1,figsize=(10, 6))\n",
    "\n",
    "    x_grid = np.linspace(-xrange*xtcalibrationfactor*FACTOR, xrange*xtcalibrationfactor*FACTOR, xrange * 2)\n",
    "\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    # Current profile is relative to zeta; transpose so that the drive bunch center agrees with the fit.\n",
    "    \n",
    "    if isChargePV:\n",
    "        # Normalize predicted image so that its integral matches the charge\n",
    "        charge_value = charge_filtered[ntest[idx]]\n",
    "        pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params, dtype=torch.float32).to(device)).cpu().detach().numpy().reshape(2*xrange, 2*yrange)\n",
    "    else:\n",
    "        pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params, dtype=torch.float32).to(device)).cpu().detach().numpy().reshape(2*xrange, 2*yrange)\n",
    "    pred_im = pred_im.T\n",
    "    pred_im_proj = np.sum(pred_im, axis = 0) * FACTOR\n",
    "    ax0.plot(x_grid, pred_im_proj, label = \"prediction\")\n",
    "    # Fix x-axis limits\n",
    "    ax0.set(xlim = (-xrange * xtcalibrationfactor * FACTOR, xrange * xtcalibrationfactor * FACTOR))\n",
    "    ax0.set(ylim=(1e2, None))\n",
    "    ax0.legend()\n",
    "    # cbar.set_label(\"Current [arb. units]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "\n",
    "    # Display image_to_bigaussian_params debug info\n",
    "    #biGaussianTest = image_to_bigaussian_params(images[valid_rows][ntest[idx]].reshape((2*yrange, 2*xrange)), do_current_profile, debug=True)\n",
    "    #print(\"Bi-Gaussian Parameters for Test Image:\", biGaussianTest)\n",
    "    \n",
    "\n",
    "# Create slider\n",
    "interact(plot_xtcav_image_pred_current, idx=IntSlider(min=0, max=pred_test_full.shape[0]-1, step=1, value=0, layout=Layout(width='80%')));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOS2 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_data = analyze_eos_and_cher(data_struct, experiment=experiment, runname=runname, skipEOSanalysis=False, EOS2ymin=50, EOS2ymax=250, mindels=90e-6, maxdels=140e-6, goosing=True, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS2horzProj = eos_data[\"EOS2horzProj\"]\n",
    "\n",
    "def find_current_profile_peak_separation(idx, debug = False):\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params, dtype=torch.float32).to(device)).cpu().detach().numpy().reshape(2*xrange, 2*yrange)\n",
    "    # Current profile is relative to zeta; transpose so that the drive bunch center agrees with the fit.\n",
    "    pred_im = pred_im.T\n",
    "    pred_im = gaussian_filter(pred_im, sigma=2)\n",
    "    pred_im_proj = np.sum(pred_im, axis = 1)\n",
    "    # Find peaks\n",
    "    max = np.max(pred_im_proj)\n",
    "    peaks = scipy.signal.find_peaks(pred_im_proj, height=max*0.05, prominence=max * 0.1)[0]\n",
    "    if len(peaks) >= 2:\n",
    "        try:\n",
    "            # Fit bigaussian function\n",
    "            x_coords = np.arange(2*xrange)\n",
    "            # Initial guess for bigaussian fit: [amp1, sigma1, mean1, amp2, sigma2, mean2]\n",
    "            # Sigma is chosen as 10 pixels arbitrarily. Amplitudes are peak heights.\n",
    "            p0_x = [x_coords[peaks[0]], 10 ,pred_im_proj[peaks[0]], x_coords[peaks[1]], 10 ,pred_im_proj[peaks[1]]]\n",
    "            popt_x, _ = curve_fit(bigaussian_1d, x_coords, pred_im_proj, p0=p0_x, maxfev=5000)\n",
    "            peak_separation = popt_x[3] - popt_x[0]\n",
    "            time_separation_fs = peak_separation * (xtcalibrationfactor * 1e15)\n",
    "            if debug:\n",
    "                # Plot for debugging\n",
    "                fig, ax = plt.subplots(figsize=(8, 4))\n",
    "                x_grid = np.linspace(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, xrange * 2)\n",
    "                ax.plot(x_grid, pred_im_proj*1e15, label = \"prediction\")\n",
    "                ax.plot(x_grid[peaks], pred_im_proj[peaks]*1e15, \"x\", label=\"peaks\")\n",
    "                ax.plot(x_grid, bigaussian_1d(x_coords, popt_x[0], popt_x[1], popt_x[2], popt_x[3], popt_x[4], popt_x[5])*1e15, label = \"fit\")\n",
    "                ax.set(xlim = (-xrange * xtcalibrationfactor * 1e15, xrange * xtcalibrationfactor * 1e15))\n",
    "                ax.set(ylim=(1e2, None))\n",
    "                ax.set_title(f'Current Profile with Peaks: Separation = {time_separation_fs:.2f} fs')\n",
    "                ax.legend()\n",
    "                plt.show()\n",
    "            return time_separation_fs\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "def plot_eos2_projection(idx):\n",
    "    fig, (axr, ax, axc, ax2, cx1) = plt.subplots(1, 5, figsize=(14, 4), gridspec_kw={'width_ratios': [1, 1, 1, 1, 0.02]})\n",
    "    eos2_proj = EOS2horzProj[:, idx]\n",
    "    axr.imshow(eos_data[\"shotROI\"][:,:,idx])\n",
    "    axr.set(xlim = (100, 175))\n",
    "    axr.set(ylim=(75, 200))\n",
    "    ax.plot(np.log(1+np.flip(eos2_proj)), label='EOS2 Horizontal Projection')\n",
    "    ax.set_xlabel('Time [a.u.]')\n",
    "    ax.set_ylabel('Log Intensity [arb. units]')\n",
    "    ax.set_title(f'EOS2 Horizontal Projection: Shot Number: {ntest[idx]}')\n",
    "    # Dels in text\n",
    "    ax.text(0.5, 0.9, f'Dels: {eos_data[\"dels\"][idx]*1e6:.2f} um', transform=ax.transAxes, ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    # bc14BLEN in text\n",
    "    ax.text(0.5, 0.8, f'BC14 BLEN: {eos_data[\"bc14BLEN\"][idx]:.2f}', transform=ax.transAxes, ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.set(xlim = (200, 350))\n",
    "    ax.set(ylim=(8, None))\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    pred_im = model_cvae.decode_latent_mu(torch.tensor(pred_params, dtype=torch.float32).to(device)).cpu().detach().numpy().reshape(2*xrange, 2*yrange)\n",
    "    # Normalize predicted image so that its integral matches the charge\n",
    "    if isChargePV:\n",
    "        charge_value = charge_filtered[ntest[idx]]\n",
    "        pred_im = pred_im * (charge_value / np.sum(pred_im))\n",
    "    # smooth predicted image\n",
    "    pred_im = gaussian_filter(pred_im, sigma=2)\n",
    "    im2 = ax2.imshow(pred_im*1e15, cmap = \"jet\", extent=(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, 0, 2*yrange), aspect='auto')\n",
    "    ax2.set(xlabel = \"Time [fs]\")\n",
    "    ax2.set(ylabel = \"y [pix]\")\n",
    "    ax2.set(title = f\"LPS Prediction: {experiment}_{runname}, Shot Number: {ntest[idx]}\")\n",
    "\n",
    "    # Plot the current profile from LPS prediction\n",
    "    current_profile = np.sum(pred_im, axis=0) * 1e15  # Sum over y to get current profile\n",
    "    # Apply Gaussian smoothing to the current profile\n",
    "    current_profile = gaussian_filter(current_profile, sigma=2)\n",
    "    x_grid = np.linspace(-xrange*xtcalibrationfactor*1e15, xrange*xtcalibrationfactor*1e15, xrange * 2)\n",
    "    axc.plot(x_grid, current_profile, label='LPS Predicted Current Profile', color='orange')\n",
    "    axc.set(xlabel='Time [fs]', ylabel='Current [A]', title='LPS Predicted Current Profile')\n",
    "    axc.text(0.5, 0.9, f'Sep: {find_current_profile_peak_separation(idx):.2f} fs', transform=axc.transAxes, ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    cbar = fig.colorbar(im2, cax=cx1, fraction=0.16, pad=0.04)\n",
    "    cbar.set_label(\"Current [A per y pixel]\")\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    # plt.tight_layout()\n",
    "    # fig.show()\n",
    "    # Also plot R² value for this index\n",
    "    print(f\"Displaying prediction for index: {idx}\")\n",
    "    print(f\"Parameters: {pred_params}\")\n",
    "    plt.show()\n",
    "# Create slider for EOS2 projection\n",
    "# Goosing with step of 2 to skip blank shots\n",
    "interact(plot_eos2_projection, idx=IntSlider(min=1, max=EOS2horzProj.shape[1]-1, step=2, value=0, layout=Layout(width='80%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_current_profile_peak_separation for all shots and plot scatter shot separation vs dels\n",
    "peak_separations = []\n",
    "for i in range(EOS2horzProj.shape[1]):\n",
    "    sep = find_current_profile_peak_separation(i)\n",
    "    peak_separations.append(sep)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(eos_data[\"dels\"]*1e6, peak_separations, alpha=0.7)\n",
    "plt.xlabel('Dels (um)')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title('Predicted Peak Separation vs EOS2 Dels')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 220)\n",
    "plt.show()\n",
    "print(peak_separations[399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of dels vs BC14 BLEN\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(eos_data[\"bc14BLEN\"], eos_data[\"dels\"]*1e6, alpha=0.5)\n",
    "plt.xlabel('BC14 BLEN')\n",
    "plt.ylabel('Dels [um]')\n",
    "plt.ylim(50, 200)\n",
    "plt.xlim(10000, 30000)\n",
    "plt.title(f'EOS2 Measured Dels vs BC14 BLEN: {experiment}_{runname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of BC14 BLEN vs predicted peak separation\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(eos_data[\"bc14BLEN\"], peak_separations, alpha=0.5)\n",
    "r_squared = np.corrcoef(eos_data[\"bc14BLEN\"], peak_separations)[0, 1]**2\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "plt.xlabel('BC14 BLEN')\n",
    "plt.ylabel('Predicted Peak Separation (fs)')\n",
    "plt.title(f'Predicted Peak Separation vs BC14 BLEN: {experiment}_{runname}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syag_data = analyze_SYAG(data_struct, experiment=experiment, runname=runname, skipEOSanalysis=False, SYAGxmin=800, SYAGxmax=950, mindels=300, maxdels=500, goosing=True, debug = False, step_selector=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(syag_data[\"dels\"])\n",
    "\n",
    "# Plot dels with blen.\n",
    "# valid indexes are the ones where dels is non-zero\n",
    "valid_idxs = syag_data[\"dels\"] != 0.0\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(eos_data[\"bc14BLEN\"][valid_idxs], syag_data[\"dels\"][valid_idxs]*1e6, alpha=0.5)\n",
    "plt.xlabel('BC14 BLEN')\n",
    "plt.ylabel('SYAG Dels [um]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtcav_image_mu(idx):\n",
    "    FACTOR = 1e15\n",
    "\n",
    "    pred_params = pred_test_full.T.reshape(NCOMP,pred_test_full.shape[0])[:,idx]\n",
    "    return pred_params\n",
    "mu_list = []\n",
    "for i in range(500):\n",
    "    mu_list.append(xtcav_image_mu(i))\n",
    "plt.plot(mu_list)\n",
    "plt.xlabel('Shot Index')\n",
    "plt.ylabel('Distance between Centroids (fs)')\n",
    "plt.title('Predicted Latent Z Components in {}'.format(f\"{experiment}_{runname}\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of PV Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bsaScalarData TCAV_LI20_2400_P and TCAV_LI20_2400_A, most important predictors.\n",
    "bsaVarNames = bsaVars\n",
    "var1_name = 'TCAV_LI20_2400_P'\n",
    "var2_name = 'TCAV_LI20_2400_A'\n",
    "var1_idx = bsaVarNames.index(var1_name)\n",
    "var2_idx = bsaVarNames.index(var2_name)\n",
    "print(f\"Plotting BSA Scalars: {var1_name} (index {var1_idx}) and {var2_name} (index {var2_idx})\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(predictor_tmp[:, var1_idx], predictor_tmp[:, var2_idx], alpha=0.5)\n",
    "plt.xlabel(var1_name)\n",
    "plt.ylabel(var2_name)\n",
    "plt.title('BSA Scalar Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming bsaScalarData has shape (N_variables, N_samples) from your function\n",
    "# Transpose the data so features are columns and samples are rows for scikit-learn PCA\n",
    "X = bsaScalarData.T \n",
    "\n",
    "# Apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Determine the maximum number of components to test\n",
    "# X.shape is (N_samples, N_variables). max_components = min(N_samples, N_variables) - 1 for a stable PCA\n",
    "max_components = min(X.shape) // 5 \n",
    "\n",
    "# Lists to store results\n",
    "n_components_list = []\n",
    "reconstruction_losses = []\n",
    "\n",
    "# Loop through possible number of components\n",
    "for k in range(1, max_components + 1):\n",
    "    # 1. Initialize and fit PCA\n",
    "    pca_study = PCA(n_components=k)\n",
    "    pca_study.fit(X)\n",
    "    \n",
    "    # 2. Transform and Inverse Transform (Reconstruct)\n",
    "    X_reduced = pca_study.transform(X)\n",
    "    X_reconstructed = pca_study.inverse_transform(X_reduced)\n",
    "    \n",
    "    # 3. Calculate Reconstruction Loss (Mean Squared Error)\n",
    "    loss = mean_squared_error(X, X_reconstructed)\n",
    "    \n",
    "    # Store results\n",
    "    n_components_list.append(k)\n",
    "    reconstruction_losses.append(loss)\n",
    "    \n",
    "# 4. Plot the Results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(n_components_list, reconstruction_losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Reconstruction Loss (Mean Squared Error)')\n",
    "# Log scale in y-axis for better visualization\n",
    "plt.yscale('log')\n",
    "plt.title('PCA of BSA Scalars: Reconstruction Loss vs. Number of Components')\n",
    "plt.grid(True)\n",
    "# 5. Identify the \"Elbow\" point visually after plotting plt.show()\n",
    "plt.show() \n",
    "\n",
    "# After plotting, the optimal number of components is the 'elbow' point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_components = 11\n",
    "\n",
    "pca_comp_study = PCA(n_components=num_components)\n",
    "pca_comp_study.fit(X)\n",
    "# 1. Get explained variance and component loadings\n",
    "\n",
    "variance_ratios = pca_comp_study.explained_variance_ratio_\n",
    "loadings = pca_comp_study.components_\n",
    "\n",
    "# 2. Create the DataFrame for component composition\n",
    "# Each row in 'loadings' is a principal component (PC)\n",
    "# Each column corresponds to a feature (PV)\n",
    "df_loadings = pd.DataFrame(loadings, columns=bsaVars)\n",
    "\n",
    "# 3. Add Component labels and Significance\n",
    "component_labels = [f'PC {i+1}' for i in range(num_components)]\n",
    "df_loadings.insert(0, 'Component', component_labels)\n",
    "df_loadings.insert(1, 'Significance (Explained Variance Ratio)', variance_ratios)\n",
    "\n",
    "# 4. Format the output\n",
    "# The components are already ordered by significance (PC 1 is most significant)\n",
    "# Format the significance column as a percentage for clarity\n",
    "df_loadings['Significance (Explained Variance Ratio)'] = \\\n",
    "    df_loadings['Significance (Explained Variance Ratio)'].map(lambda x: f'{x:.4f} ({x*100:.2f}%)')\n",
    "\n",
    "# Format the loadings to a fixed number of decimal places\n",
    "for col in bsaVars:\n",
    "    df_loadings[col] = df_loadings[col].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "composition_table = df_loadings.to_markdown(index=False)\n",
    "print(\"Composition of Principal Components:\")\n",
    "print(composition_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract and Square the Loadings (Loadings matrix is: Components x Variables)\n",
    "# loadings = pca_comp_study.components_ (from your provided code block)\n",
    "squared_loadings = loadings**2\n",
    "\n",
    "# 2. Sum Across Components (Sum columns to get total significance per variable)\n",
    "# The result is an array where each element is the total squared loading for a variable\n",
    "total_squared_loadings = np.sum(squared_loadings, axis=0)\n",
    "\n",
    "# 3. Create a DataFrame for sorting\n",
    "# bsaVars is the list of variable names (features)\n",
    "df_var_significance = pd.DataFrame({\n",
    "    'BSA Variable (PV)': bsaVars,\n",
    "    'Total Squared Loading (Significance)': total_squared_loadings\n",
    "})\n",
    "\n",
    "# 4. Sort in descending order\n",
    "df_var_significance = df_var_significance.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 5. Specify the number of top variables to display\n",
    "N_top = 20  # Example: display the top few most significant variables\n",
    "\n",
    "# 6. Format and display the table\n",
    "df_top_vars = df_var_significance.head(N_top)\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "print(f\"Top {N_top} BSA Variables Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df_var_significance[\n",
    "    ~df_var_significance['BSA Variable (PV)'].str.contains('BPM', case=False, na=False)\n",
    "]\n",
    "\n",
    "# 5. Sort the filtered variables in descending order of significance\n",
    "df_filtered = df_filtered.sort_values(\n",
    "    by='Total Squared Loading (Significance)', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 6. Select Top N and format\n",
    "N_top = 20 # Display the top few most significant non-BPM variables\n",
    "df_top_vars = df_filtered.head(N_top)\n",
    "\n",
    "# Format the significance column\n",
    "df_top_vars['Total Squared Loading (Significance)'] = \\\n",
    "    df_top_vars['Total Squared Loading (Significance)'].map(lambda x: f'{x:.4f}')\n",
    "\n",
    "# Display the resulting table\n",
    "inverted_table = df_top_vars.to_markdown(index=False)\n",
    "\n",
    "print(f\"Top {N_top} BSA Variables (EXCLUDING BPMS) Ordered by Significance in PCA Components:\")\n",
    "print(inverted_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtcav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
